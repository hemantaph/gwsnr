{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psds not given. Choosing bilby's default psds\n",
      "npool:  4\n",
      "snr type:  ann\n",
      "waveform approximant:  IMRPhenomXPHM\n",
      "sampling frequency:  2048.0\n",
      "minimum frequency (fmin):  20.0\n",
      "mtot=mass1+mass2\n",
      "min(mtot):  2.0\n",
      "max(mtot) (with the given fmin=20.0): 184.98599853446768\n",
      "detectors:  None\n",
      "ANN method is selected.\n",
      "Please be patient while the interpolator is generated of partialscaledSNR for IMRPhenomD.\n",
      "Interpolator will be loaded for L1 detector from ./interpolator_pickle/L1/partialSNR_dict_1.pickle\n",
      "Interpolator will be loaded for H1 detector from ./interpolator_pickle/H1/partialSNR_dict_1.pickle\n",
      "Interpolator will be loaded for V1 detector from ./interpolator_pickle/V1/partialSNR_dict_1.pickle\n"
     ]
    }
   ],
   "source": [
    "from gwsnr import GWSNR\n",
    "gwsnr = GWSNR(snr_type='ann', waveform_approximant='IMRPhenomXPHM', pdet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': array([0, 0, 1, 0]),\n",
       " 'H1': array([0, 0, 0, 0]),\n",
       " 'V1': array([0, 0, 0, 0]),\n",
       " 'pdet_net': array([1, 0, 1, 0])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mass_1 = np.array([5, 10.,50.,200.])\n",
    "ratio = np.array([1, 0.8,0.5,0.2])\n",
    "luminosity_distance = np.array([1000, 2000, 3000, 4000])\n",
    "# gwsnr.snr_with_ann(mass_1=mass_1, mass_2=mass_1*ratio, luminosity_distance=luminosity_distance)\n",
    "gwsnr.snr(mass_1=mass_1, mass_2=mass_1*ratio, luminosity_distance=luminosity_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'L1': array([0, 0, 1, 0]),\n",
       " 'H1': array([0, 0, 0, 0]),\n",
       " 'V1': array([0, 0, 0, 0]),\n",
       " 'pdet_net': array([1, 0, 1, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snr_bilby = gwsnr.compute_bilby_snr(mass_1=mass_1, mass_2=mass_1*ratio, luminosity_distance=luminosity_distance)\n",
    "gwsnr.probability_of_detection(snr_dict=snr_bilby, type='bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n",
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "# check tensorflow version\n",
    "print(tf.__version__)\n",
    "# check sklearn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "from ler.utils import append_json, load_json\n",
    "import numpy as np\n",
    "unlensed_params = load_json(\"snr_L1.json\")\n",
    "# unlensed_params = load_json(\"joint_new_optsnr.json\")\n",
    "# for key, value in unlensed_params.items():\n",
    "#     unlensed_params[key] = np.array(value)\n",
    "# randomize the data\n",
    "idx_shuffle = np.random.permutation(len(unlensed_params['L1']))\n",
    "for key, value in unlensed_params.items():\n",
    "    unlensed_params[key] = np.array(value)[idx_shuffle]\n",
    "#snr = np.array(unlensed_params['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28113"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlensed_params['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test = np.array(unlensed_params['L1'])\n",
    "test = test[test<20]\n",
    "plt.hist(test, bins=30)\n",
    "plt.xlim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psds not given. Choosing bilby's default psds\n",
      "npool:  4\n",
      "snr type:  ann\n",
      "waveform approximant:  IMRPhenomXPHM\n",
      "sampling frequency:  2048.0\n",
      "minimum frequency (fmin):  20.0\n",
      "mtot=mass1+mass2\n",
      "min(mtot):  2.0\n",
      "max(mtot) (with the given fmin=20.0): 184.98599853446768\n",
      "detectors:  None\n",
      "ANN method is selected.\n",
      "Please be patient while the interpolator is generated of partialscaledSNR for IMRPhenomD.\n",
      "Interpolator will be loaded for L1 detector from ./interpolator_pickle/L1/partialSNR_dict_1.pickle\n",
      "Interpolator will be loaded for H1 detector from ./interpolator_pickle/H1/partialSNR_dict_1.pickle\n",
      "Interpolator will be loaded for V1 detector from ./interpolator_pickle/V1/partialSNR_dict_1.pickle\n"
     ]
    }
   ],
   "source": [
    "# let's generate IMRPhenomD (spinless) interpolartor\n",
    "from gwsnr import GWSNR\n",
    "gwsnr = GWSNR(snr_type='ann', waveform_approximant='IMRPhenomXPHM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwsnr import antenna_response_array, cubic_spline_interpolator2d\n",
    "\n",
    "def input_output(idx, params):\n",
    "\n",
    "    mass_1 = np.array(params['mass_1'])[idx]\n",
    "    mass_2 = np.array(params['mass_2'])[idx]\n",
    "    luminosity_distance = np.array(params['luminosity_distance'])[idx]\n",
    "    theta_jn = np.array(params['theta_jn'])[idx]\n",
    "    psi = np.array(params['psi'])[idx]\n",
    "    geocent_time = np.array(params['geocent_time'])[idx]\n",
    "    ra = np.array(params['ra'])[idx]\n",
    "    dec = np.array(params['dec'])[idx]\n",
    "    \n",
    "    detector_tensor = gwsnr.detector_tensor_list\n",
    "    snr_halfscaled = np.array(gwsnr.snr_partialsacaled_list)\n",
    "    ratio_arr = gwsnr.ratio_arr\n",
    "    mtot_arr = gwsnr.mtot_arr\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    len_ = len(detector_tensor)\n",
    "    mtot = mass_1 + mass_2\n",
    "    ratio = mass_2 / mass_1\n",
    "    # get array of antenna response\n",
    "    Fp, Fc = antenna_response_array(ra, dec, geocent_time, psi, detector_tensor)\n",
    "\n",
    "    Mc = ((mass_1 * mass_2) ** (3 / 5)) / ((mass_1 + mass_2) ** (1 / 5))\n",
    "    eta = mass_1 * mass_2/(mass_1 + mass_2)**2.\n",
    "    A1 = Mc ** (5.0 / 6.0)\n",
    "    ci_2 = np.cos(theta_jn) ** 2\n",
    "    ci_param = ((1 + np.cos(theta_jn) ** 2) / 2) ** 2\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    snr_half_ = np.zeros((len_,size))\n",
    "    d_eff = np.zeros((len_,size))\n",
    "\n",
    "    # loop over the detectors\n",
    "    for j in range(len_):\n",
    "        # loop over the parameter points\n",
    "        for i in range(size):\n",
    "            snr_half_coeff = snr_halfscaled[j]\n",
    "            snr_half_[j,i] = cubic_spline_interpolator2d(mtot[i], ratio[i], snr_half_coeff, mtot_arr, ratio_arr)\n",
    "            d_eff[j,i] =luminosity_distance[i] / np.sqrt(\n",
    "                    Fp[j,i]**2 * ci_param[i] + Fc[j,i]**2 * ci_2[i]\n",
    "                )\n",
    "\n",
    "    #amp0\n",
    "    amp0 =  A1 / d_eff\n",
    "\n",
    "    # get spin parameters\n",
    "    a_1 = np.array(params['a_1'])[idx]\n",
    "    a_2 = np.array(params['a_2'])[idx]\n",
    "    tilt_1 = np.array(params['tilt_1'])[idx]\n",
    "    tilt_2 = np.array(params['tilt_2'])[idx]\n",
    "    phi_12 = np.array(params['phi_12'])[idx]\n",
    "    phi_jl = np.array(params['phi_jl'])[idx]\n",
    "\n",
    "    # input data\n",
    "    # X = np.vstack([L1, amp0, Mc, eta, theta_jn, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    XL1 = np.vstack([snr_half_[0], amp0[0], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    XH1 = np.vstack([snr_half_[1], amp0[1], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    XV1 = np.vstack([snr_half_[2], amp0[2], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "\n",
    "    # output data\n",
    "    # get L1 snr for y train \n",
    "    yL1 = params['L1'][idx]\n",
    "    yH1 = params['H1'][idx]\n",
    "    yV1 = params['V1'][idx]\n",
    "\n",
    "\n",
    "    return(XL1, XH1, XV1, yL1, yH1, yV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28113, 9)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ = len(unlensed_params['L1'])\n",
    "idx = np.arange(len_)\n",
    "# randomize the train set\n",
    "idx = np.random.permutation(idx)\n",
    "XL1, XH1, XV1, yL1, yH1, yV1 = input_output(idx, unlensed_params)\n",
    "np.shape(XL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(y[y<100], bins=100)\n",
    "# plt.xlim(0,20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now back to ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XL1, yL1, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(X_train))\n",
    "# print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler\n",
    "import pickle\n",
    "pickle.dump(sc, open('scalerL1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "ann = tf.keras.models.Sequential() \n",
    "\n",
    "# adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=9, activation='relu'))\n",
    "# adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "# adding the third hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n",
    "# adding the output layer, absolute value of the snr\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "791/791 [==============================] - 1s 497us/step - loss: 63.4480 - accuracy: 2.3714e-04\n",
      "Epoch 2/200\n",
      "791/791 [==============================] - 0s 498us/step - loss: 24.7348 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "791/791 [==============================] - 0s 496us/step - loss: 16.6748 - accuracy: 3.9524e-05\n",
      "Epoch 4/200\n",
      "791/791 [==============================] - 0s 486us/step - loss: 12.5886 - accuracy: 7.9048e-05\n",
      "Epoch 5/200\n",
      "791/791 [==============================] - 0s 487us/step - loss: 10.0020 - accuracy: 7.9048e-05\n",
      "Epoch 6/200\n",
      "791/791 [==============================] - 0s 487us/step - loss: 8.2039 - accuracy: 1.1857e-04\n",
      "Epoch 7/200\n",
      "791/791 [==============================] - 0s 515us/step - loss: 6.8957 - accuracy: 1.1857e-04\n",
      "Epoch 8/200\n",
      "791/791 [==============================] - 0s 500us/step - loss: 5.9086 - accuracy: 1.1857e-04\n",
      "Epoch 9/200\n",
      "791/791 [==============================] - 0s 493us/step - loss: 5.1250 - accuracy: 1.5810e-04\n",
      "Epoch 10/200\n",
      "791/791 [==============================] - 0s 492us/step - loss: 4.5190 - accuracy: 3.1619e-04\n",
      "Epoch 11/200\n",
      "791/791 [==============================] - 0s 486us/step - loss: 4.0349 - accuracy: 7.9048e-05\n",
      "Epoch 12/200\n",
      "791/791 [==============================] - 0s 491us/step - loss: 3.6503 - accuracy: 2.7667e-04\n",
      "Epoch 13/200\n",
      "791/791 [==============================] - 0s 492us/step - loss: 3.3384 - accuracy: 1.9762e-04\n",
      "Epoch 14/200\n",
      "791/791 [==============================] - 0s 527us/step - loss: 3.0449 - accuracy: 1.5810e-04\n",
      "Epoch 15/200\n",
      "791/791 [==============================] - 0s 499us/step - loss: 2.8314 - accuracy: 2.7667e-04\n",
      "Epoch 16/200\n",
      "791/791 [==============================] - 0s 494us/step - loss: 2.6347 - accuracy: 3.1619e-04\n",
      "Epoch 17/200\n",
      "791/791 [==============================] - 0s 491us/step - loss: 2.4780 - accuracy: 2.3714e-04\n",
      "Epoch 18/200\n",
      "791/791 [==============================] - 0s 500us/step - loss: 2.3431 - accuracy: 3.1619e-04\n",
      "Epoch 19/200\n",
      "791/791 [==============================] - 0s 493us/step - loss: 2.2015 - accuracy: 1.9762e-04\n",
      "Epoch 20/200\n",
      "791/791 [==============================] - 0s 521us/step - loss: 2.0696 - accuracy: 3.5572e-04\n",
      "Epoch 21/200\n",
      "791/791 [==============================] - 0s 500us/step - loss: 1.9763 - accuracy: 2.3714e-04\n",
      "Epoch 22/200\n",
      "791/791 [==============================] - 0s 500us/step - loss: 1.8676 - accuracy: 3.1619e-04\n",
      "Epoch 23/200\n",
      "791/791 [==============================] - 0s 495us/step - loss: 1.7600 - accuracy: 3.9524e-04\n",
      "Epoch 24/200\n",
      "791/791 [==============================] - 0s 494us/step - loss: 1.6988 - accuracy: 4.7429e-04\n",
      "Epoch 25/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 1.6431 - accuracy: 3.1619e-04\n",
      "Epoch 26/200\n",
      "791/791 [==============================] - 0s 542us/step - loss: 1.5545 - accuracy: 2.7667e-04\n",
      "Epoch 27/200\n",
      "791/791 [==============================] - 0s 501us/step - loss: 1.5045 - accuracy: 3.1619e-04\n",
      "Epoch 28/200\n",
      "791/791 [==============================] - 0s 495us/step - loss: 1.4655 - accuracy: 3.1619e-04\n",
      "Epoch 29/200\n",
      "791/791 [==============================] - 0s 510us/step - loss: 1.4277 - accuracy: 3.9524e-04\n",
      "Epoch 30/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 1.3607 - accuracy: 2.3714e-04\n",
      "Epoch 31/200\n",
      "791/791 [==============================] - 0s 535us/step - loss: 1.3362 - accuracy: 2.3714e-04\n",
      "Epoch 32/200\n",
      "791/791 [==============================] - 0s 516us/step - loss: 1.3102 - accuracy: 2.3714e-04\n",
      "Epoch 33/200\n",
      "791/791 [==============================] - 0s 496us/step - loss: 1.2938 - accuracy: 3.5572e-04\n",
      "Epoch 34/200\n",
      "791/791 [==============================] - 0s 531us/step - loss: 1.2724 - accuracy: 3.1619e-04\n",
      "Epoch 35/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 1.2328 - accuracy: 1.5810e-04\n",
      "Epoch 36/200\n",
      "791/791 [==============================] - 0s 495us/step - loss: 1.2072 - accuracy: 3.5572e-04\n",
      "Epoch 37/200\n",
      "791/791 [==============================] - 0s 492us/step - loss: 1.1946 - accuracy: 4.7429e-04\n",
      "Epoch 38/200\n",
      "791/791 [==============================] - 0s 518us/step - loss: 1.2136 - accuracy: 3.1619e-04\n",
      "Epoch 39/200\n",
      "791/791 [==============================] - 0s 491us/step - loss: 1.1743 - accuracy: 2.3714e-04\n",
      "Epoch 40/200\n",
      "791/791 [==============================] - 0s 488us/step - loss: 1.1486 - accuracy: 3.1619e-04\n",
      "Epoch 41/200\n",
      "791/791 [==============================] - 0s 495us/step - loss: 1.1395 - accuracy: 4.3477e-04\n",
      "Epoch 42/200\n",
      "791/791 [==============================] - 0s 499us/step - loss: 1.1228 - accuracy: 3.5572e-04\n",
      "Epoch 43/200\n",
      "791/791 [==============================] - 0s 492us/step - loss: 1.0998 - accuracy: 2.3714e-04\n",
      "Epoch 44/200\n",
      "791/791 [==============================] - 0s 495us/step - loss: 1.0972 - accuracy: 5.1381e-04\n",
      "Epoch 45/200\n",
      "791/791 [==============================] - 0s 534us/step - loss: 1.1463 - accuracy: 2.7667e-04\n",
      "Epoch 46/200\n",
      "791/791 [==============================] - 0s 516us/step - loss: 1.1073 - accuracy: 3.1619e-04\n",
      "Epoch 47/200\n",
      "791/791 [==============================] - 0s 544us/step - loss: 1.0718 - accuracy: 4.7429e-04\n",
      "Epoch 48/200\n",
      "791/791 [==============================] - 0s 520us/step - loss: 1.0922 - accuracy: 3.9524e-04\n",
      "Epoch 49/200\n",
      "791/791 [==============================] - 0s 522us/step - loss: 1.0817 - accuracy: 3.1619e-04\n",
      "Epoch 50/200\n",
      "791/791 [==============================] - 0s 501us/step - loss: 1.0675 - accuracy: 4.3477e-04\n",
      "Epoch 51/200\n",
      "791/791 [==============================] - 0s 498us/step - loss: 1.0461 - accuracy: 3.9524e-04\n",
      "Epoch 52/200\n",
      "791/791 [==============================] - 0s 532us/step - loss: 1.0687 - accuracy: 2.7667e-04\n",
      "Epoch 53/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 1.0337 - accuracy: 3.9524e-04\n",
      "Epoch 54/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 1.0263 - accuracy: 3.5572e-04\n",
      "Epoch 55/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 1.0557 - accuracy: 3.9524e-04\n",
      "Epoch 56/200\n",
      "791/791 [==============================] - 0s 497us/step - loss: 1.0223 - accuracy: 2.7667e-04\n",
      "Epoch 57/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 1.0657 - accuracy: 3.1619e-04\n",
      "Epoch 58/200\n",
      "791/791 [==============================] - 0s 541us/step - loss: 1.0258 - accuracy: 4.3477e-04\n",
      "Epoch 59/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 1.0612 - accuracy: 3.9524e-04\n",
      "Epoch 60/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 1.0378 - accuracy: 5.1381e-04\n",
      "Epoch 61/200\n",
      "791/791 [==============================] - 0s 510us/step - loss: 1.0181 - accuracy: 3.5572e-04\n",
      "Epoch 62/200\n",
      "791/791 [==============================] - 0s 514us/step - loss: 1.0242 - accuracy: 5.5334e-04\n",
      "Epoch 63/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 1.0576 - accuracy: 5.1381e-04\n",
      "Epoch 64/200\n",
      "791/791 [==============================] - 0s 532us/step - loss: 1.0102 - accuracy: 3.5572e-04\n",
      "Epoch 65/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 1.0058 - accuracy: 4.3477e-04\n",
      "Epoch 66/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 1.0145 - accuracy: 4.3477e-04\n",
      "Epoch 67/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 1.0044 - accuracy: 3.1619e-04\n",
      "Epoch 68/200\n",
      "791/791 [==============================] - 0s 499us/step - loss: 1.0360 - accuracy: 3.9524e-04\n",
      "Epoch 69/200\n",
      "791/791 [==============================] - 0s 534us/step - loss: 1.0313 - accuracy: 3.9524e-04\n",
      "Epoch 70/200\n",
      "791/791 [==============================] - 0s 514us/step - loss: 0.9915 - accuracy: 4.7429e-04\n",
      "Epoch 71/200\n",
      "791/791 [==============================] - 0s 501us/step - loss: 1.0245 - accuracy: 3.9524e-04\n",
      "Epoch 72/200\n",
      "791/791 [==============================] - 0s 497us/step - loss: 1.0144 - accuracy: 4.3477e-04\n",
      "Epoch 73/200\n",
      "791/791 [==============================] - 0s 508us/step - loss: 1.0044 - accuracy: 3.9524e-04\n",
      "Epoch 74/200\n",
      "791/791 [==============================] - 0s 510us/step - loss: 1.0064 - accuracy: 3.5572e-04\n",
      "Epoch 75/200\n",
      "791/791 [==============================] - 0s 538us/step - loss: 1.0082 - accuracy: 2.7667e-04\n",
      "Epoch 76/200\n",
      "791/791 [==============================] - 0s 501us/step - loss: 0.9992 - accuracy: 6.3239e-04\n",
      "Epoch 77/200\n",
      "791/791 [==============================] - 0s 515us/step - loss: 1.0017 - accuracy: 3.9524e-04\n",
      "Epoch 78/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 0.9985 - accuracy: 3.5572e-04\n",
      "Epoch 79/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 1.0028 - accuracy: 3.9524e-04\n",
      "Epoch 80/200\n",
      "791/791 [==============================] - 0s 529us/step - loss: 0.9835 - accuracy: 3.9524e-04\n",
      "Epoch 81/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 1.0097 - accuracy: 4.3477e-04\n",
      "Epoch 82/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.9908 - accuracy: 3.5572e-04\n",
      "Epoch 83/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 1.0009 - accuracy: 5.5334e-04\n",
      "Epoch 84/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.9611 - accuracy: 4.7429e-04\n",
      "Epoch 85/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.9758 - accuracy: 3.1619e-04\n",
      "Epoch 86/200\n",
      "791/791 [==============================] - 0s 542us/step - loss: 0.9892 - accuracy: 4.3477e-04\n",
      "Epoch 87/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 0.9630 - accuracy: 3.5572e-04\n",
      "Epoch 88/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 0.9830 - accuracy: 3.5572e-04\n",
      "Epoch 89/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.9861 - accuracy: 3.9524e-04\n",
      "Epoch 90/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.9683 - accuracy: 3.5572e-04\n",
      "Epoch 91/200\n",
      "791/791 [==============================] - 0s 513us/step - loss: 0.9605 - accuracy: 5.5334e-04\n",
      "Epoch 92/200\n",
      "791/791 [==============================] - 0s 534us/step - loss: 0.9889 - accuracy: 3.1619e-04\n",
      "Epoch 93/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 0.9788 - accuracy: 3.5572e-04\n",
      "Epoch 94/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.9727 - accuracy: 4.3477e-04\n",
      "Epoch 95/200\n",
      "791/791 [==============================] - 0s 515us/step - loss: 0.9738 - accuracy: 3.1619e-04\n",
      "Epoch 96/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.9660 - accuracy: 3.9524e-04\n",
      "Epoch 97/200\n",
      "791/791 [==============================] - 0s 537us/step - loss: 0.9758 - accuracy: 4.3477e-04\n",
      "Epoch 98/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 0.9655 - accuracy: 4.7429e-04\n",
      "Epoch 99/200\n",
      "791/791 [==============================] - 0s 509us/step - loss: 0.9539 - accuracy: 3.5572e-04\n",
      "Epoch 100/200\n",
      "791/791 [==============================] - 0s 511us/step - loss: 0.9708 - accuracy: 3.5572e-04\n",
      "Epoch 101/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.9482 - accuracy: 2.7667e-04\n",
      "Epoch 102/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.9595 - accuracy: 3.9524e-04\n",
      "Epoch 103/200\n",
      "791/791 [==============================] - 0s 545us/step - loss: 0.9728 - accuracy: 3.5572e-04\n",
      "Epoch 104/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.9528 - accuracy: 3.1619e-04\n",
      "Epoch 105/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.9556 - accuracy: 5.5334e-04\n",
      "Epoch 106/200\n",
      "791/791 [==============================] - 0s 511us/step - loss: 0.9314 - accuracy: 4.3477e-04\n",
      "Epoch 107/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.9517 - accuracy: 4.3477e-04\n",
      "Epoch 108/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.9627 - accuracy: 2.3714e-04\n",
      "Epoch 109/200\n",
      "791/791 [==============================] - 0s 535us/step - loss: 0.9334 - accuracy: 3.9524e-04\n",
      "Epoch 110/200\n",
      "791/791 [==============================] - 0s 517us/step - loss: 0.9463 - accuracy: 3.9524e-04\n",
      "Epoch 111/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.9348 - accuracy: 3.1619e-04\n",
      "Epoch 112/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 0.9514 - accuracy: 4.3477e-04\n",
      "Epoch 113/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.9401 - accuracy: 5.1381e-04\n",
      "Epoch 114/200\n",
      "791/791 [==============================] - 0s 547us/step - loss: 0.9311 - accuracy: 1.9762e-04\n",
      "Epoch 115/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 0.9427 - accuracy: 3.5572e-04\n",
      "Epoch 116/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.9234 - accuracy: 5.5334e-04\n",
      "Epoch 117/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.9036 - accuracy: 3.1619e-04\n",
      "Epoch 118/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.9261 - accuracy: 3.9524e-04\n",
      "Epoch 119/200\n",
      "791/791 [==============================] - 0s 527us/step - loss: 0.9112 - accuracy: 2.7667e-04\n",
      "Epoch 120/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.9342 - accuracy: 3.5572e-04\n",
      "Epoch 121/200\n",
      "791/791 [==============================] - 0s 515us/step - loss: 0.9340 - accuracy: 4.3477e-04\n",
      "Epoch 122/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.9110 - accuracy: 3.9524e-04\n",
      "Epoch 123/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.9162 - accuracy: 5.5334e-04\n",
      "Epoch 124/200\n",
      "791/791 [==============================] - 0s 541us/step - loss: 0.9014 - accuracy: 3.1619e-04\n",
      "Epoch 125/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.9242 - accuracy: 5.5334e-04\n",
      "Epoch 126/200\n",
      "791/791 [==============================] - 0s 508us/step - loss: 0.9065 - accuracy: 4.3477e-04\n",
      "Epoch 127/200\n",
      "791/791 [==============================] - 0s 513us/step - loss: 0.8999 - accuracy: 3.1619e-04\n",
      "Epoch 128/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 0.9038 - accuracy: 4.3477e-04\n",
      "Epoch 129/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 0.9186 - accuracy: 4.7429e-04\n",
      "Epoch 130/200\n",
      "791/791 [==============================] - 0s 535us/step - loss: 0.8948 - accuracy: 3.5572e-04\n",
      "Epoch 131/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.8762 - accuracy: 3.5572e-04\n",
      "Epoch 132/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8868 - accuracy: 4.3477e-04\n",
      "Epoch 133/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.9038 - accuracy: 3.9524e-04\n",
      "Epoch 134/200\n",
      "791/791 [==============================] - 0s 501us/step - loss: 0.8879 - accuracy: 3.9524e-04\n",
      "Epoch 135/200\n",
      "791/791 [==============================] - 0s 526us/step - loss: 0.8776 - accuracy: 2.7667e-04\n",
      "Epoch 136/200\n",
      "791/791 [==============================] - 0s 510us/step - loss: 0.8998 - accuracy: 3.5572e-04\n",
      "Epoch 137/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8866 - accuracy: 4.7429e-04\n",
      "Epoch 138/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.8745 - accuracy: 3.9524e-04\n",
      "Epoch 139/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.9092 - accuracy: 4.3477e-04\n",
      "Epoch 140/200\n",
      "791/791 [==============================] - 0s 530us/step - loss: 0.8771 - accuracy: 5.5334e-04\n",
      "Epoch 141/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.8635 - accuracy: 4.3477e-04\n",
      "Epoch 142/200\n",
      "791/791 [==============================] - 0s 517us/step - loss: 0.8728 - accuracy: 3.1619e-04\n",
      "Epoch 143/200\n",
      "791/791 [==============================] - 0s 508us/step - loss: 0.8696 - accuracy: 4.3477e-04\n",
      "Epoch 144/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.8662 - accuracy: 3.5572e-04\n",
      "Epoch 145/200\n",
      "791/791 [==============================] - 0s 540us/step - loss: 0.8667 - accuracy: 4.3477e-04\n",
      "Epoch 146/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 0.8626 - accuracy: 3.1619e-04\n",
      "Epoch 147/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 0.8496 - accuracy: 4.3477e-04\n",
      "Epoch 148/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.8764 - accuracy: 5.1381e-04\n",
      "Epoch 149/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.8537 - accuracy: 4.7429e-04\n",
      "Epoch 150/200\n",
      "791/791 [==============================] - 0s 545us/step - loss: 0.8573 - accuracy: 4.3477e-04\n",
      "Epoch 151/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.8415 - accuracy: 4.7429e-04\n",
      "Epoch 152/200\n",
      "791/791 [==============================] - 0s 511us/step - loss: 0.8892 - accuracy: 4.3477e-04\n",
      "Epoch 153/200\n",
      "791/791 [==============================] - 0s 515us/step - loss: 0.8477 - accuracy: 3.9524e-04\n",
      "Epoch 154/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 0.8370 - accuracy: 5.1381e-04\n",
      "Epoch 155/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.8524 - accuracy: 3.9524e-04\n",
      "Epoch 156/200\n",
      "791/791 [==============================] - 0s 542us/step - loss: 0.8628 - accuracy: 3.5572e-04\n",
      "Epoch 157/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.8369 - accuracy: 5.1381e-04\n",
      "Epoch 158/200\n",
      "791/791 [==============================] - 0s 524us/step - loss: 0.8599 - accuracy: 3.5572e-04\n",
      "Epoch 159/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8307 - accuracy: 4.7429e-04\n",
      "Epoch 160/200\n",
      "791/791 [==============================] - 0s 536us/step - loss: 0.8464 - accuracy: 2.7667e-04\n",
      "Epoch 161/200\n",
      "791/791 [==============================] - 0s 514us/step - loss: 0.8617 - accuracy: 4.3477e-04\n",
      "Epoch 162/200\n",
      "791/791 [==============================] - 0s 502us/step - loss: 0.8440 - accuracy: 4.3477e-04\n",
      "Epoch 163/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.8351 - accuracy: 3.9524e-04\n",
      "Epoch 164/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.8441 - accuracy: 3.5572e-04\n",
      "Epoch 165/200\n",
      "791/791 [==============================] - 0s 536us/step - loss: 0.8201 - accuracy: 4.7429e-04\n",
      "Epoch 166/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.8213 - accuracy: 5.1381e-04\n",
      "Epoch 167/200\n",
      "791/791 [==============================] - 0s 506us/step - loss: 0.8737 - accuracy: 3.5572e-04\n",
      "Epoch 168/200\n",
      "791/791 [==============================] - 0s 517us/step - loss: 0.8142 - accuracy: 4.3477e-04\n",
      "Epoch 169/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.8212 - accuracy: 3.5572e-04\n",
      "Epoch 170/200\n",
      "791/791 [==============================] - 0s 544us/step - loss: 0.8399 - accuracy: 3.1619e-04\n",
      "Epoch 171/200\n",
      "791/791 [==============================] - 0s 503us/step - loss: 0.8344 - accuracy: 3.5572e-04\n",
      "Epoch 172/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8263 - accuracy: 3.5572e-04\n",
      "Epoch 173/200\n",
      "791/791 [==============================] - 0s 514us/step - loss: 0.8311 - accuracy: 3.5572e-04\n",
      "Epoch 174/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8274 - accuracy: 3.1619e-04\n",
      "Epoch 175/200\n",
      "791/791 [==============================] - 0s 544us/step - loss: 0.8125 - accuracy: 3.5572e-04\n",
      "Epoch 176/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.8381 - accuracy: 3.5572e-04\n",
      "Epoch 177/200\n",
      "791/791 [==============================] - 0s 517us/step - loss: 0.8230 - accuracy: 4.3477e-04\n",
      "Epoch 178/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8149 - accuracy: 5.1381e-04\n",
      "Epoch 179/200\n",
      "791/791 [==============================] - 0s 508us/step - loss: 0.8196 - accuracy: 3.5572e-04\n",
      "Epoch 180/200\n",
      "791/791 [==============================] - 0s 536us/step - loss: 0.8177 - accuracy: 5.1381e-04\n",
      "Epoch 181/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.8242 - accuracy: 3.9524e-04\n",
      "Epoch 182/200\n",
      "791/791 [==============================] - 0s 511us/step - loss: 0.8165 - accuracy: 4.7429e-04\n",
      "Epoch 183/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8250 - accuracy: 4.7429e-04\n",
      "Epoch 184/200\n",
      "791/791 [==============================] - 0s 517us/step - loss: 0.8185 - accuracy: 3.1619e-04\n",
      "Epoch 185/200\n",
      "791/791 [==============================] - 0s 533us/step - loss: 0.8166 - accuracy: 5.1381e-04\n",
      "Epoch 186/200\n",
      "791/791 [==============================] - 0s 514us/step - loss: 0.8135 - accuracy: 3.9524e-04\n",
      "Epoch 187/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.8159 - accuracy: 3.5572e-04\n",
      "Epoch 188/200\n",
      "791/791 [==============================] - 0s 513us/step - loss: 0.8068 - accuracy: 3.1619e-04\n",
      "Epoch 189/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.8252 - accuracy: 3.9524e-04\n",
      "Epoch 190/200\n",
      "791/791 [==============================] - 0s 534us/step - loss: 0.7986 - accuracy: 3.1619e-04\n",
      "Epoch 191/200\n",
      "791/791 [==============================] - 0s 514us/step - loss: 0.8017 - accuracy: 4.7429e-04\n",
      "Epoch 192/200\n",
      "791/791 [==============================] - 0s 498us/step - loss: 0.8042 - accuracy: 3.9524e-04\n",
      "Epoch 193/200\n",
      "791/791 [==============================] - 0s 512us/step - loss: 0.8036 - accuracy: 5.1381e-04\n",
      "Epoch 194/200\n",
      "791/791 [==============================] - 0s 507us/step - loss: 0.8007 - accuracy: 4.3477e-04\n",
      "Epoch 195/200\n",
      "791/791 [==============================] - 0s 552us/step - loss: 0.8063 - accuracy: 3.1619e-04\n",
      "Epoch 196/200\n",
      "791/791 [==============================] - 0s 504us/step - loss: 0.8010 - accuracy: 3.5572e-04\n",
      "Epoch 197/200\n",
      "791/791 [==============================] - 0s 513us/step - loss: 0.8091 - accuracy: 3.1619e-04\n",
      "Epoch 198/200\n",
      "791/791 [==============================] - 0s 505us/step - loss: 0.8177 - accuracy: 4.7429e-04\n",
      "Epoch 199/200\n",
      "791/791 [==============================] - 0s 517us/step - loss: 0.7982 - accuracy: 4.7429e-04\n",
      "Epoch 200/200\n",
      "791/791 [==============================] - 0s 531us/step - loss: 0.7892 - accuracy: 5.1381e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c785db70>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the ANN on the training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 200, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 566us/step\n",
      "[[ 4.61121416  4.77777499]\n",
      " [ 7.59788942  8.27535942]\n",
      " [10.89720917 10.26178774]\n",
      " [ 8.52272606  8.20943627]\n",
      " [15.47688866 14.06081837]\n",
      " [16.4499836  16.66089685]\n",
      " [ 2.7054565   2.72369782]\n",
      " [10.51429176 11.22310476]\n",
      " [10.42868805 10.33807745]\n",
      " [ 6.90547132  7.4488752 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2.20%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred.flatten()>6) != (y_test>6))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812\n",
      "1057\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "# y_test = Y_\n",
    "hist_ = y_pred-y_test\n",
    "print(len(hist_))\n",
    "idx = (y_test>4) & (y_test<10)\n",
    "hist_ = hist_[idx]\n",
    "#hist_ = hist_[abs(hist_)<5.]\n",
    "print(len(hist_))\n",
    "plt.figure(figsize=(4,4)) \n",
    "plt.hist(hist_, bins=100, histtype='step', density=True)\n",
    "plt.xlim(-5,5)\n",
    "plt.xlabel('snr_ml and snr_inner_product difference')\n",
    "plt.ylabel('pdf')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "ann.save('ann_modelL1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XH1, yH1, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# save the scaler\n",
    "import pickle\n",
    "pickle.dump(sc, open('scalerH1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "ann = tf.keras.models.Sequential() \n",
    "\n",
    "# adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=9, activation='relu'))\n",
    "# adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "# adding the third hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n",
    "# adding the output layer, absolute value of the snr\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the ANN\n",
    "# loss = 'mean_squared_error'\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2813/2813 [==============================] - 2s 502us/step - loss: 5.9933 - accuracy: 1.3332e-04\n",
      "Epoch 2/200\n",
      "2813/2813 [==============================] - 1s 519us/step - loss: 1.8604 - accuracy: 2.7776e-04\n",
      "Epoch 3/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 1.2152 - accuracy: 2.8887e-04\n",
      "Epoch 4/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.9227 - accuracy: 3.2220e-04\n",
      "Epoch 5/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.7461 - accuracy: 3.1109e-04\n",
      "Epoch 6/200\n",
      "2813/2813 [==============================] - 1s 508us/step - loss: 0.6484 - accuracy: 2.9998e-04\n",
      "Epoch 7/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.5772 - accuracy: 3.3331e-04\n",
      "Epoch 8/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.5270 - accuracy: 3.5553e-04\n",
      "Epoch 9/200\n",
      "2813/2813 [==============================] - 1s 504us/step - loss: 0.4927 - accuracy: 3.1109e-04\n",
      "Epoch 10/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.4717 - accuracy: 3.3331e-04\n",
      "Epoch 11/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.4412 - accuracy: 3.5553e-04\n",
      "Epoch 12/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.4334 - accuracy: 4.1108e-04\n",
      "Epoch 13/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.4063 - accuracy: 3.8886e-04\n",
      "Epoch 14/200\n",
      "2813/2813 [==============================] - 1s 520us/step - loss: 0.4011 - accuracy: 3.6664e-04\n",
      "Epoch 15/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.3907 - accuracy: 3.8886e-04\n",
      "Epoch 16/200\n",
      "2813/2813 [==============================] - 1s 509us/step - loss: 0.3837 - accuracy: 4.2219e-04\n",
      "Epoch 17/200\n",
      "2813/2813 [==============================] - 1s 504us/step - loss: 0.3717 - accuracy: 3.7775e-04\n",
      "Epoch 18/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.3646 - accuracy: 4.3330e-04\n",
      "Epoch 19/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.3560 - accuracy: 4.2219e-04\n",
      "Epoch 20/200\n",
      "2813/2813 [==============================] - 1s 511us/step - loss: 0.3496 - accuracy: 4.5553e-04\n",
      "Epoch 21/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.3436 - accuracy: 4.6664e-04\n",
      "Epoch 22/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.3434 - accuracy: 4.7775e-04\n",
      "Epoch 23/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.3327 - accuracy: 4.6664e-04\n",
      "Epoch 24/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.3311 - accuracy: 4.7775e-04\n",
      "Epoch 25/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.3284 - accuracy: 5.2219e-04\n",
      "Epoch 26/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.3225 - accuracy: 4.8886e-04\n",
      "Epoch 27/200\n",
      "2813/2813 [==============================] - 1s 490us/step - loss: 0.3244 - accuracy: 5.2219e-04\n",
      "Epoch 28/200\n",
      "2813/2813 [==============================] - 1s 478us/step - loss: 0.3189 - accuracy: 5.9996e-04\n",
      "Epoch 29/200\n",
      "2813/2813 [==============================] - 1s 497us/step - loss: 0.3220 - accuracy: 6.8884e-04\n",
      "Epoch 30/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.3123 - accuracy: 7.4439e-04\n",
      "Epoch 31/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.3184 - accuracy: 6.6662e-04\n",
      "Epoch 32/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.3126 - accuracy: 7.5551e-04\n",
      "Epoch 33/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.3092 - accuracy: 8.1106e-04\n",
      "Epoch 34/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.3091 - accuracy: 7.9995e-04\n",
      "Epoch 35/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.3100 - accuracy: 7.7773e-04\n",
      "Epoch 36/200\n",
      "2813/2813 [==============================] - 1s 478us/step - loss: 0.3065 - accuracy: 8.6661e-04\n",
      "Epoch 37/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.3116 - accuracy: 8.5550e-04\n",
      "Epoch 38/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2995 - accuracy: 8.7772e-04\n",
      "Epoch 39/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2998 - accuracy: 8.8883e-04\n",
      "Epoch 40/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.3073 - accuracy: 9.2216e-04\n",
      "Epoch 41/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2978 - accuracy: 9.4438e-04\n",
      "Epoch 42/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.3011 - accuracy: 9.2216e-04\n",
      "Epoch 43/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.3011 - accuracy: 9.6660e-04\n",
      "Epoch 44/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.3017 - accuracy: 0.0010\n",
      "Epoch 45/200\n",
      "2813/2813 [==============================] - 1s 474us/step - loss: 0.2959 - accuracy: 0.0011\n",
      "Epoch 46/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2971 - accuracy: 0.0011\n",
      "Epoch 47/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.3015 - accuracy: 0.0012\n",
      "Epoch 48/200\n",
      "2813/2813 [==============================] - 1s 490us/step - loss: 0.2929 - accuracy: 0.0012\n",
      "Epoch 49/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2949 - accuracy: 0.0012\n",
      "Epoch 50/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2925 - accuracy: 0.0013\n",
      "Epoch 51/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2924 - accuracy: 0.0014\n",
      "Epoch 52/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2983 - accuracy: 0.0014\n",
      "Epoch 53/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2902 - accuracy: 0.0014\n",
      "Epoch 54/200\n",
      "2813/2813 [==============================] - 1s 486us/step - loss: 0.2904 - accuracy: 0.0015\n",
      "Epoch 55/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2914 - accuracy: 0.0014\n",
      "Epoch 56/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2967 - accuracy: 0.0015\n",
      "Epoch 57/200\n",
      "2813/2813 [==============================] - 1s 471us/step - loss: 0.2885 - accuracy: 0.0016\n",
      "Epoch 58/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2895 - accuracy: 0.0016\n",
      "Epoch 59/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2890 - accuracy: 0.0017\n",
      "Epoch 60/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2851 - accuracy: 0.0018\n",
      "Epoch 61/200\n",
      "2813/2813 [==============================] - 1s 477us/step - loss: 0.2823 - accuracy: 0.0018\n",
      "Epoch 62/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2887 - accuracy: 0.0019\n",
      "Epoch 63/200\n",
      "2813/2813 [==============================] - 1s 473us/step - loss: 0.2851 - accuracy: 0.0019\n",
      "Epoch 64/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.2894 - accuracy: 0.0020\n",
      "Epoch 65/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2790 - accuracy: 0.0021\n",
      "Epoch 66/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2852 - accuracy: 0.0022\n",
      "Epoch 67/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2889 - accuracy: 0.0023\n",
      "Epoch 68/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.2862 - accuracy: 0.0024\n",
      "Epoch 69/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2762 - accuracy: 0.0027\n",
      "Epoch 70/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2768 - accuracy: 0.0029\n",
      "Epoch 71/200\n",
      "2813/2813 [==============================] - 1s 477us/step - loss: 0.2795 - accuracy: 0.0030\n",
      "Epoch 72/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.2778 - accuracy: 0.0031\n",
      "Epoch 73/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2752 - accuracy: 0.0032\n",
      "Epoch 74/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.2744 - accuracy: 0.0034\n",
      "Epoch 75/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2739 - accuracy: 0.0035\n",
      "Epoch 76/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2741 - accuracy: 0.0034\n",
      "Epoch 77/200\n",
      "2813/2813 [==============================] - 1s 490us/step - loss: 0.2764 - accuracy: 0.0036\n",
      "Epoch 78/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.2694 - accuracy: 0.0036\n",
      "Epoch 79/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2726 - accuracy: 0.0036\n",
      "Epoch 80/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2747 - accuracy: 0.0037\n",
      "Epoch 81/200\n",
      "2813/2813 [==============================] - 1s 477us/step - loss: 0.2718 - accuracy: 0.0037\n",
      "Epoch 82/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2782 - accuracy: 0.0037\n",
      "Epoch 83/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2679 - accuracy: 0.0038\n",
      "Epoch 84/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2709 - accuracy: 0.0038\n",
      "Epoch 85/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2672 - accuracy: 0.0039\n",
      "Epoch 86/200\n",
      "2813/2813 [==============================] - 1s 478us/step - loss: 0.2704 - accuracy: 0.0038\n",
      "Epoch 87/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.2796 - accuracy: 0.0040\n",
      "Epoch 88/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2708 - accuracy: 0.0039\n",
      "Epoch 89/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2696 - accuracy: 0.0039\n",
      "Epoch 90/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2661 - accuracy: 0.0040\n",
      "Epoch 91/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2705 - accuracy: 0.0040\n",
      "Epoch 92/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2725 - accuracy: 0.0040\n",
      "Epoch 93/200\n",
      "2813/2813 [==============================] - 1s 477us/step - loss: 0.2673 - accuracy: 0.0040\n",
      "Epoch 94/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2740 - accuracy: 0.0039\n",
      "Epoch 95/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2638 - accuracy: 0.0042\n",
      "Epoch 96/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2691 - accuracy: 0.0039\n",
      "Epoch 97/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2720 - accuracy: 0.0041\n",
      "Epoch 98/200\n",
      "2813/2813 [==============================] - 1s 474us/step - loss: 0.2668 - accuracy: 0.0041\n",
      "Epoch 99/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.2679 - accuracy: 0.0041\n",
      "Epoch 100/200\n",
      "2813/2813 [==============================] - 1s 516us/step - loss: 0.2679 - accuracy: 0.0042\n",
      "Epoch 101/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.2636 - accuracy: 0.0041\n",
      "Epoch 102/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.2716 - accuracy: 0.0041\n",
      "Epoch 103/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2646 - accuracy: 0.0042\n",
      "Epoch 104/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.2659 - accuracy: 0.0041\n",
      "Epoch 105/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2634 - accuracy: 0.0042\n",
      "Epoch 106/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2714 - accuracy: 0.0041\n",
      "Epoch 107/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.2627 - accuracy: 0.0044\n",
      "Epoch 108/200\n",
      "2813/2813 [==============================] - 1s 515us/step - loss: 0.2676 - accuracy: 0.0042\n",
      "Epoch 109/200\n",
      "2813/2813 [==============================] - 1s 509us/step - loss: 0.2627 - accuracy: 0.0042\n",
      "Epoch 110/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.2702 - accuracy: 0.0043\n",
      "Epoch 111/200\n",
      "2813/2813 [==============================] - 1s 508us/step - loss: 0.2631 - accuracy: 0.0041\n",
      "Epoch 112/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.2700 - accuracy: 0.0043\n",
      "Epoch 113/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2698 - accuracy: 0.0042\n",
      "Epoch 114/200\n",
      "2813/2813 [==============================] - 1s 473us/step - loss: 0.2691 - accuracy: 0.0044\n",
      "Epoch 115/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2670 - accuracy: 0.0042\n",
      "Epoch 116/200\n",
      "2813/2813 [==============================] - 1s 473us/step - loss: 0.2645 - accuracy: 0.0042\n",
      "Epoch 117/200\n",
      "2813/2813 [==============================] - 1s 486us/step - loss: 0.2666 - accuracy: 0.0041\n",
      "Epoch 118/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.2646 - accuracy: 0.0041\n",
      "Epoch 119/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.2637 - accuracy: 0.0043\n",
      "Epoch 120/200\n",
      "2813/2813 [==============================] - 1s 509us/step - loss: 0.2676 - accuracy: 0.0042\n",
      "Epoch 121/200\n",
      "2813/2813 [==============================] - 1s 512us/step - loss: 0.2633 - accuracy: 0.0042\n",
      "Epoch 122/200\n",
      "2813/2813 [==============================] - 1s 505us/step - loss: 0.2665 - accuracy: 0.0041\n",
      "Epoch 123/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.2699 - accuracy: 0.0042\n",
      "Epoch 124/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.2632 - accuracy: 0.0044\n",
      "Epoch 125/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2660 - accuracy: 0.0042\n",
      "Epoch 126/200\n",
      "2813/2813 [==============================] - 1s 478us/step - loss: 0.2611 - accuracy: 0.0044\n",
      "Epoch 127/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2645 - accuracy: 0.0043\n",
      "Epoch 128/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2615 - accuracy: 0.0043\n",
      "Epoch 129/200\n",
      "2813/2813 [==============================] - 1s 474us/step - loss: 0.2658 - accuracy: 0.0043\n",
      "Epoch 130/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2637 - accuracy: 0.0043\n",
      "Epoch 131/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2605 - accuracy: 0.0042\n",
      "Epoch 132/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2655 - accuracy: 0.0042\n",
      "Epoch 133/200\n",
      "2813/2813 [==============================] - 1s 486us/step - loss: 0.2645 - accuracy: 0.0044\n",
      "Epoch 134/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2598 - accuracy: 0.0044\n",
      "Epoch 135/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.2624 - accuracy: 0.0043\n",
      "Epoch 136/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2628 - accuracy: 0.0042\n",
      "Epoch 137/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2606 - accuracy: 0.0043\n",
      "Epoch 138/200\n",
      "2813/2813 [==============================] - 1s 490us/step - loss: 0.2666 - accuracy: 0.0044\n",
      "Epoch 139/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2604 - accuracy: 0.0043\n",
      "Epoch 140/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2655 - accuracy: 0.0044\n",
      "Epoch 141/200\n",
      "2813/2813 [==============================] - 1s 474us/step - loss: 0.2599 - accuracy: 0.0046\n",
      "Epoch 142/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.2624 - accuracy: 0.0044\n",
      "Epoch 143/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2576 - accuracy: 0.0043\n",
      "Epoch 144/200\n",
      "2813/2813 [==============================] - 1s 505us/step - loss: 0.2601 - accuracy: 0.0043\n",
      "Epoch 145/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.2612 - accuracy: 0.0043\n",
      "Epoch 146/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2599 - accuracy: 0.0045\n",
      "Epoch 147/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2704 - accuracy: 0.0044\n",
      "Epoch 148/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2606 - accuracy: 0.0044\n",
      "Epoch 149/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.2607 - accuracy: 0.0043\n",
      "Epoch 150/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.2617 - accuracy: 0.0044\n",
      "Epoch 151/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.2630 - accuracy: 0.0045\n",
      "Epoch 152/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2598 - accuracy: 0.0044\n",
      "Epoch 153/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.2603 - accuracy: 0.0042\n",
      "Epoch 154/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2604 - accuracy: 0.0045\n",
      "Epoch 155/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.2617 - accuracy: 0.0044\n",
      "Epoch 156/200\n",
      "2813/2813 [==============================] - 1s 497us/step - loss: 0.2691 - accuracy: 0.0045\n",
      "Epoch 157/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2645 - accuracy: 0.0044\n",
      "Epoch 158/200\n",
      "2813/2813 [==============================] - 1s 478us/step - loss: 0.2616 - accuracy: 0.0044\n",
      "Epoch 159/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.2599 - accuracy: 0.0044\n",
      "Epoch 160/200\n",
      "2813/2813 [==============================] - 1s 472us/step - loss: 0.2601 - accuracy: 0.0044\n",
      "Epoch 161/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.2598 - accuracy: 0.0044\n",
      "Epoch 162/200\n",
      "2813/2813 [==============================] - 1s 497us/step - loss: 0.2621 - accuracy: 0.0046\n",
      "Epoch 163/200\n",
      "2813/2813 [==============================] - 1s 512us/step - loss: 0.2608 - accuracy: 0.0045\n",
      "Epoch 164/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.2599 - accuracy: 0.0042\n",
      "Epoch 165/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2666 - accuracy: 0.0043\n",
      "Epoch 166/200\n",
      "2813/2813 [==============================] - 1s 471us/step - loss: 0.2584 - accuracy: 0.0046\n",
      "Epoch 167/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.2577 - accuracy: 0.0044\n",
      "Epoch 168/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2625 - accuracy: 0.0045\n",
      "Epoch 169/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.2571 - accuracy: 0.0044\n",
      "Epoch 170/200\n",
      "2813/2813 [==============================] - 1s 481us/step - loss: 0.2606 - accuracy: 0.0044\n",
      "Epoch 171/200\n",
      "2813/2813 [==============================] - 1s 471us/step - loss: 0.2601 - accuracy: 0.0045\n",
      "Epoch 172/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2603 - accuracy: 0.0044\n",
      "Epoch 173/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2688 - accuracy: 0.0043\n",
      "Epoch 174/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2674 - accuracy: 0.0043\n",
      "Epoch 175/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2585 - accuracy: 0.0045\n",
      "Epoch 176/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2622 - accuracy: 0.0045\n",
      "Epoch 177/200\n",
      "2813/2813 [==============================] - 1s 469us/step - loss: 0.2586 - accuracy: 0.0045\n",
      "Epoch 178/200\n",
      "2813/2813 [==============================] - 1s 478us/step - loss: 0.2591 - accuracy: 0.0044\n",
      "Epoch 179/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2587 - accuracy: 0.0045\n",
      "Epoch 180/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.2604 - accuracy: 0.0045\n",
      "Epoch 181/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2591 - accuracy: 0.0045\n",
      "Epoch 182/200\n",
      "2813/2813 [==============================] - 1s 479us/step - loss: 0.2596 - accuracy: 0.0044\n",
      "Epoch 183/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.2603 - accuracy: 0.0045\n",
      "Epoch 184/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.2617 - accuracy: 0.0045\n",
      "Epoch 185/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.2599 - accuracy: 0.0045\n",
      "Epoch 186/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.2598 - accuracy: 0.0045\n",
      "Epoch 187/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.2650 - accuracy: 0.0045\n",
      "Epoch 188/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2582 - accuracy: 0.0045\n",
      "Epoch 189/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.2609 - accuracy: 0.0046\n",
      "Epoch 190/200\n",
      "2813/2813 [==============================] - 1s 486us/step - loss: 0.2572 - accuracy: 0.0044\n",
      "Epoch 191/200\n",
      "2813/2813 [==============================] - 1s 482us/step - loss: 0.2624 - accuracy: 0.0044\n",
      "Epoch 192/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2593 - accuracy: 0.0046\n",
      "Epoch 193/200\n",
      "2813/2813 [==============================] - 1s 480us/step - loss: 0.2593 - accuracy: 0.0044\n",
      "Epoch 194/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2609 - accuracy: 0.0044\n",
      "Epoch 195/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2593 - accuracy: 0.0044\n",
      "Epoch 196/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.2568 - accuracy: 0.0046\n",
      "Epoch 197/200\n",
      "2813/2813 [==============================] - 1s 475us/step - loss: 0.2574 - accuracy: 0.0045\n",
      "Epoch 198/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2616 - accuracy: 0.0043\n",
      "Epoch 199/200\n",
      "2813/2813 [==============================] - 1s 486us/step - loss: 0.2601 - accuracy: 0.0044\n",
      "Epoch 200/200\n",
      "2813/2813 [==============================] - 1s 476us/step - loss: 0.2580 - accuracy: 0.0045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d3eb56f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the ANN on the training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 200, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 338us/step\n",
      "[[0.54871368 0.4168334 ]\n",
      " [2.4766047  2.12355028]\n",
      " [8.95738316 8.69730996]\n",
      " [6.19180012 5.69036995]\n",
      " [8.43137741 8.34367143]\n",
      " [8.10725498 8.23281797]\n",
      " [9.75430393 9.88966943]\n",
      " [0.77356434 0.79247159]\n",
      " [0.68909764 0.73560906]\n",
      " [7.59835768 7.79408989]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 3.13%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred.flatten()>8) != (y_test>8))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "ann.save('ann_modelH1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XV1, yV1, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# save the scaler\n",
    "import pickle\n",
    "pickle.dump(sc, open('scalerV1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "ann = tf.keras.models.Sequential() \n",
    "\n",
    "# adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=9, activation='relu'))\n",
    "# adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "# adding the third hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n",
    "# adding the output layer, absolute value of the snr\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the ANN\n",
    "# loss = 'mean_squared_error'\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2813/2813 [==============================] - 2s 504us/step - loss: 2.4578 - accuracy: 3.5553e-04\n",
      "Epoch 2/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.9121 - accuracy: 5.9996e-04\n",
      "Epoch 3/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.6503 - accuracy: 6.6662e-04\n",
      "Epoch 4/200\n",
      "2813/2813 [==============================] - 1s 486us/step - loss: 0.5245 - accuracy: 6.2218e-04\n",
      "Epoch 5/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.4424 - accuracy: 6.5551e-04\n",
      "Epoch 6/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.4008 - accuracy: 6.3329e-04\n",
      "Epoch 7/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.3720 - accuracy: 7.2217e-04\n",
      "Epoch 8/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.3524 - accuracy: 5.7774e-04\n",
      "Epoch 9/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.3309 - accuracy: 6.8884e-04\n",
      "Epoch 10/200\n",
      "2813/2813 [==============================] - 1s 485us/step - loss: 0.3172 - accuracy: 7.4439e-04\n",
      "Epoch 11/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.3080 - accuracy: 6.4440e-04\n",
      "Epoch 12/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2975 - accuracy: 6.9995e-04\n",
      "Epoch 13/200\n",
      "2813/2813 [==============================] - 2s 537us/step - loss: 0.2870 - accuracy: 7.4439e-04\n",
      "Epoch 14/200\n",
      "2813/2813 [==============================] - 1s 525us/step - loss: 0.2832 - accuracy: 7.4439e-04\n",
      "Epoch 15/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.2778 - accuracy: 8.2217e-04\n",
      "Epoch 16/200\n",
      "2813/2813 [==============================] - 1s 508us/step - loss: 0.2736 - accuracy: 7.9995e-04\n",
      "Epoch 17/200\n",
      "2813/2813 [==============================] - 1s 527us/step - loss: 0.2681 - accuracy: 8.3328e-04\n",
      "Epoch 18/200\n",
      "2813/2813 [==============================] - 1s 505us/step - loss: 0.2632 - accuracy: 8.3328e-04\n",
      "Epoch 19/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2602 - accuracy: 8.4439e-04\n",
      "Epoch 20/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2556 - accuracy: 8.4439e-04\n",
      "Epoch 21/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2578 - accuracy: 8.1106e-04\n",
      "Epoch 22/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.2519 - accuracy: 8.5550e-04\n",
      "Epoch 23/200\n",
      "2813/2813 [==============================] - 1s 520us/step - loss: 0.2555 - accuracy: 8.6661e-04\n",
      "Epoch 24/200\n",
      "2813/2813 [==============================] - 1s 524us/step - loss: 0.2455 - accuracy: 8.6661e-04\n",
      "Epoch 25/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.2531 - accuracy: 8.4439e-04\n",
      "Epoch 26/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.2468 - accuracy: 9.1105e-04\n",
      "Epoch 27/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2467 - accuracy: 8.9994e-04\n",
      "Epoch 28/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.2396 - accuracy: 8.3328e-04\n",
      "Epoch 29/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2472 - accuracy: 9.1105e-04\n",
      "Epoch 30/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.2407 - accuracy: 9.8882e-04\n",
      "Epoch 31/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2391 - accuracy: 9.7771e-04\n",
      "Epoch 32/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2381 - accuracy: 9.8882e-04\n",
      "Epoch 33/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.2338 - accuracy: 9.8882e-04\n",
      "Epoch 34/200\n",
      "2813/2813 [==============================] - 1s 490us/step - loss: 0.2355 - accuracy: 0.0010\n",
      "Epoch 35/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2342 - accuracy: 0.0011\n",
      "Epoch 36/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2288 - accuracy: 0.0011\n",
      "Epoch 37/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.2303 - accuracy: 0.0011\n",
      "Epoch 38/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.2316 - accuracy: 0.0011\n",
      "Epoch 39/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2271 - accuracy: 0.0011\n",
      "Epoch 40/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2281 - accuracy: 0.0011\n",
      "Epoch 41/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.2250 - accuracy: 0.0011\n",
      "Epoch 42/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2258 - accuracy: 0.0012\n",
      "Epoch 43/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2253 - accuracy: 0.0013\n",
      "Epoch 44/200\n",
      "2813/2813 [==============================] - 1s 486us/step - loss: 0.2257 - accuracy: 0.0012\n",
      "Epoch 45/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.2193 - accuracy: 0.0013\n",
      "Epoch 46/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.2202 - accuracy: 0.0014\n",
      "Epoch 47/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.2213 - accuracy: 0.0014\n",
      "Epoch 48/200\n",
      "2813/2813 [==============================] - 1s 484us/step - loss: 0.2161 - accuracy: 0.0016\n",
      "Epoch 49/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2179 - accuracy: 0.0017\n",
      "Epoch 50/200\n",
      "2813/2813 [==============================] - 1s 483us/step - loss: 0.2131 - accuracy: 0.0020\n",
      "Epoch 51/200\n",
      "2813/2813 [==============================] - 1s 490us/step - loss: 0.2115 - accuracy: 0.0023\n",
      "Epoch 52/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.2139 - accuracy: 0.0025\n",
      "Epoch 53/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.2115 - accuracy: 0.0026\n",
      "Epoch 54/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.2091 - accuracy: 0.0028\n",
      "Epoch 55/200\n",
      "2813/2813 [==============================] - 1s 533us/step - loss: 0.2124 - accuracy: 0.0029\n",
      "Epoch 56/200\n",
      "2813/2813 [==============================] - 1s 522us/step - loss: 0.2087 - accuracy: 0.0030\n",
      "Epoch 57/200\n",
      "2813/2813 [==============================] - 1s 530us/step - loss: 0.2030 - accuracy: 0.0032\n",
      "Epoch 58/200\n",
      "2813/2813 [==============================] - 1s 522us/step - loss: 0.2015 - accuracy: 0.0033\n",
      "Epoch 59/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.2030 - accuracy: 0.0034\n",
      "Epoch 60/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.1996 - accuracy: 0.0034\n",
      "Epoch 61/200\n",
      "2813/2813 [==============================] - 1s 513us/step - loss: 0.1998 - accuracy: 0.0035\n",
      "Epoch 62/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1946 - accuracy: 0.0036\n",
      "Epoch 63/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.1987 - accuracy: 0.0037\n",
      "Epoch 64/200\n",
      "2813/2813 [==============================] - 1s 513us/step - loss: 0.2010 - accuracy: 0.0034\n",
      "Epoch 65/200\n",
      "2813/2813 [==============================] - 1s 512us/step - loss: 0.1969 - accuracy: 0.0038\n",
      "Epoch 66/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.2012 - accuracy: 0.0035\n",
      "Epoch 67/200\n",
      "2813/2813 [==============================] - 1s 512us/step - loss: 0.1941 - accuracy: 0.0036\n",
      "Epoch 68/200\n",
      "2813/2813 [==============================] - 1s 515us/step - loss: 0.1916 - accuracy: 0.0038\n",
      "Epoch 69/200\n",
      "2813/2813 [==============================] - 1s 513us/step - loss: 0.1914 - accuracy: 0.0039\n",
      "Epoch 70/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.1900 - accuracy: 0.0038\n",
      "Epoch 71/200\n",
      "2813/2813 [==============================] - 1s 508us/step - loss: 0.1877 - accuracy: 0.0040\n",
      "Epoch 72/200\n",
      "2813/2813 [==============================] - 1s 502us/step - loss: 0.1934 - accuracy: 0.0041\n",
      "Epoch 73/200\n",
      "2813/2813 [==============================] - 1s 508us/step - loss: 0.1884 - accuracy: 0.0039\n",
      "Epoch 74/200\n",
      "2813/2813 [==============================] - 1s 517us/step - loss: 0.1845 - accuracy: 0.0039\n",
      "Epoch 75/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1846 - accuracy: 0.0041\n",
      "Epoch 76/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1912 - accuracy: 0.0036\n",
      "Epoch 77/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1847 - accuracy: 0.0042\n",
      "Epoch 78/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.1829 - accuracy: 0.0041\n",
      "Epoch 79/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.1830 - accuracy: 0.0042\n",
      "Epoch 80/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.1824 - accuracy: 0.0043\n",
      "Epoch 81/200\n",
      "2813/2813 [==============================] - 1s 502us/step - loss: 0.1813 - accuracy: 0.0042\n",
      "Epoch 82/200\n",
      "2813/2813 [==============================] - 1s 504us/step - loss: 0.1819 - accuracy: 0.0039\n",
      "Epoch 83/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.1810 - accuracy: 0.0042\n",
      "Epoch 84/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.1797 - accuracy: 0.0041\n",
      "Epoch 85/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.1768 - accuracy: 0.0042\n",
      "Epoch 86/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.1789 - accuracy: 0.0042\n",
      "Epoch 87/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.1800 - accuracy: 0.0043\n",
      "Epoch 88/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1791 - accuracy: 0.0039\n",
      "Epoch 89/200\n",
      "2813/2813 [==============================] - 1s 511us/step - loss: 0.1760 - accuracy: 0.0043\n",
      "Epoch 90/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.1752 - accuracy: 0.0042\n",
      "Epoch 91/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.1768 - accuracy: 0.0043\n",
      "Epoch 92/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.1737 - accuracy: 0.0044\n",
      "Epoch 93/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.1721 - accuracy: 0.0043\n",
      "Epoch 94/200\n",
      "2813/2813 [==============================] - 1s 519us/step - loss: 0.1706 - accuracy: 0.0044\n",
      "Epoch 95/200\n",
      "2813/2813 [==============================] - 1s 520us/step - loss: 0.1722 - accuracy: 0.0044\n",
      "Epoch 96/200\n",
      "2813/2813 [==============================] - 1s 520us/step - loss: 0.1723 - accuracy: 0.0043\n",
      "Epoch 97/200\n",
      "2813/2813 [==============================] - 1s 502us/step - loss: 0.1701 - accuracy: 0.0044\n",
      "Epoch 98/200\n",
      "2813/2813 [==============================] - 1s 509us/step - loss: 0.1687 - accuracy: 0.0044\n",
      "Epoch 99/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1709 - accuracy: 0.0041\n",
      "Epoch 100/200\n",
      "2813/2813 [==============================] - 1s 513us/step - loss: 0.1709 - accuracy: 0.0044\n",
      "Epoch 101/200\n",
      "2813/2813 [==============================] - 1s 522us/step - loss: 0.1686 - accuracy: 0.0044\n",
      "Epoch 102/200\n",
      "2813/2813 [==============================] - 1s 521us/step - loss: 0.1647 - accuracy: 0.0044\n",
      "Epoch 103/200\n",
      "2813/2813 [==============================] - 2s 538us/step - loss: 0.1660 - accuracy: 0.0044\n",
      "Epoch 104/200\n",
      "2813/2813 [==============================] - 1s 520us/step - loss: 0.1676 - accuracy: 0.0042\n",
      "Epoch 105/200\n",
      "2813/2813 [==============================] - 1s 507us/step - loss: 0.1669 - accuracy: 0.0043\n",
      "Epoch 106/200\n",
      "2813/2813 [==============================] - 1s 507us/step - loss: 0.1651 - accuracy: 0.0044\n",
      "Epoch 107/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.1634 - accuracy: 0.0045\n",
      "Epoch 108/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.1627 - accuracy: 0.0044\n",
      "Epoch 109/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.1626 - accuracy: 0.0044\n",
      "Epoch 110/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1640 - accuracy: 0.0043\n",
      "Epoch 111/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.1659 - accuracy: 0.0042\n",
      "Epoch 112/200\n",
      "2813/2813 [==============================] - 1s 502us/step - loss: 0.1635 - accuracy: 0.0041\n",
      "Epoch 113/200\n",
      "2813/2813 [==============================] - 1s 509us/step - loss: 0.1605 - accuracy: 0.0045\n",
      "Epoch 114/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1596 - accuracy: 0.0044\n",
      "Epoch 115/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.1616 - accuracy: 0.0044\n",
      "Epoch 116/200\n",
      "2813/2813 [==============================] - 1s 504us/step - loss: 0.1599 - accuracy: 0.0043\n",
      "Epoch 117/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.1576 - accuracy: 0.0043\n",
      "Epoch 118/200\n",
      "2813/2813 [==============================] - 1s 502us/step - loss: 0.1578 - accuracy: 0.0045\n",
      "Epoch 119/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1572 - accuracy: 0.0046\n",
      "Epoch 120/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1595 - accuracy: 0.0046\n",
      "Epoch 121/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.1565 - accuracy: 0.0044\n",
      "Epoch 122/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1572 - accuracy: 0.0042\n",
      "Epoch 123/200\n",
      "2813/2813 [==============================] - 1s 507us/step - loss: 0.1566 - accuracy: 0.0044\n",
      "Epoch 124/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.1566 - accuracy: 0.0043\n",
      "Epoch 125/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.1531 - accuracy: 0.0045\n",
      "Epoch 126/200\n",
      "2813/2813 [==============================] - 2s 534us/step - loss: 0.1586 - accuracy: 0.0042\n",
      "Epoch 127/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.1553 - accuracy: 0.0045\n",
      "Epoch 128/200\n",
      "2813/2813 [==============================] - 1s 509us/step - loss: 0.1555 - accuracy: 0.0044\n",
      "Epoch 129/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1533 - accuracy: 0.0044\n",
      "Epoch 130/200\n",
      "2813/2813 [==============================] - 1s 507us/step - loss: 0.1532 - accuracy: 0.0041\n",
      "Epoch 131/200\n",
      "2813/2813 [==============================] - 1s 517us/step - loss: 0.1528 - accuracy: 0.0044\n",
      "Epoch 132/200\n",
      "2813/2813 [==============================] - 1s 522us/step - loss: 0.1555 - accuracy: 0.0044\n",
      "Epoch 133/200\n",
      "2813/2813 [==============================] - 1s 522us/step - loss: 0.1536 - accuracy: 0.0046\n",
      "Epoch 134/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.1520 - accuracy: 0.0044\n",
      "Epoch 135/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1511 - accuracy: 0.0045\n",
      "Epoch 136/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.1524 - accuracy: 0.0044\n",
      "Epoch 137/200\n",
      "2813/2813 [==============================] - 2s 555us/step - loss: 0.1513 - accuracy: 0.0045\n",
      "Epoch 138/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.1519 - accuracy: 0.0044\n",
      "Epoch 139/200\n",
      "2813/2813 [==============================] - 1s 507us/step - loss: 0.1512 - accuracy: 0.0045\n",
      "Epoch 140/200\n",
      "2813/2813 [==============================] - 1s 507us/step - loss: 0.1502 - accuracy: 0.0044\n",
      "Epoch 141/200\n",
      "2813/2813 [==============================] - 1s 523us/step - loss: 0.1515 - accuracy: 0.0044\n",
      "Epoch 142/200\n",
      "2813/2813 [==============================] - 1s 502us/step - loss: 0.1500 - accuracy: 0.0046\n",
      "Epoch 143/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.1518 - accuracy: 0.0042\n",
      "Epoch 144/200\n",
      "2813/2813 [==============================] - 1s 511us/step - loss: 0.1477 - accuracy: 0.0045\n",
      "Epoch 145/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.1487 - accuracy: 0.0046\n",
      "Epoch 146/200\n",
      "2813/2813 [==============================] - 1s 532us/step - loss: 0.1491 - accuracy: 0.0044\n",
      "Epoch 147/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1498 - accuracy: 0.0045\n",
      "Epoch 148/200\n",
      "2813/2813 [==============================] - 1s 505us/step - loss: 0.1504 - accuracy: 0.0045\n",
      "Epoch 149/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.1507 - accuracy: 0.0043\n",
      "Epoch 150/200\n",
      "2813/2813 [==============================] - 1s 507us/step - loss: 0.1489 - accuracy: 0.0045\n",
      "Epoch 151/200\n",
      "2813/2813 [==============================] - 1s 516us/step - loss: 0.1488 - accuracy: 0.0045\n",
      "Epoch 152/200\n",
      "2813/2813 [==============================] - 1s 511us/step - loss: 0.1512 - accuracy: 0.0041\n",
      "Epoch 153/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1491 - accuracy: 0.0043\n",
      "Epoch 154/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1457 - accuracy: 0.0043\n",
      "Epoch 155/200\n",
      "2813/2813 [==============================] - 1s 526us/step - loss: 0.1474 - accuracy: 0.0045\n",
      "Epoch 156/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.1468 - accuracy: 0.0046\n",
      "Epoch 157/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.1468 - accuracy: 0.0046\n",
      "Epoch 158/200\n",
      "2813/2813 [==============================] - 1s 505us/step - loss: 0.1455 - accuracy: 0.0046\n",
      "Epoch 159/200\n",
      "2813/2813 [==============================] - 1s 503us/step - loss: 0.1482 - accuracy: 0.0043\n",
      "Epoch 160/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.1462 - accuracy: 0.0045\n",
      "Epoch 161/200\n",
      "2813/2813 [==============================] - 1s 522us/step - loss: 0.1453 - accuracy: 0.0045\n",
      "Epoch 162/200\n",
      "2813/2813 [==============================] - 1s 519us/step - loss: 0.1453 - accuracy: 0.0046\n",
      "Epoch 163/200\n",
      "2813/2813 [==============================] - 1s 517us/step - loss: 0.1448 - accuracy: 0.0046\n",
      "Epoch 164/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.1475 - accuracy: 0.0044\n",
      "Epoch 165/200\n",
      "2813/2813 [==============================] - 1s 506us/step - loss: 0.1455 - accuracy: 0.0046\n",
      "Epoch 166/200\n",
      "2813/2813 [==============================] - 1s 500us/step - loss: 0.1438 - accuracy: 0.0046\n",
      "Epoch 167/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.1477 - accuracy: 0.0045\n",
      "Epoch 168/200\n",
      "2813/2813 [==============================] - 2s 542us/step - loss: 0.1436 - accuracy: 0.0046\n",
      "Epoch 169/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.1451 - accuracy: 0.0045\n",
      "Epoch 170/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.1437 - accuracy: 0.0044\n",
      "Epoch 171/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.1441 - accuracy: 0.0045\n",
      "Epoch 172/200\n",
      "2813/2813 [==============================] - 1s 501us/step - loss: 0.1443 - accuracy: 0.0045\n",
      "Epoch 173/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1443 - accuracy: 0.0045\n",
      "Epoch 174/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.1450 - accuracy: 0.0047\n",
      "Epoch 175/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1455 - accuracy: 0.0045\n",
      "Epoch 176/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1445 - accuracy: 0.0046\n",
      "Epoch 177/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1442 - accuracy: 0.0044\n",
      "Epoch 178/200\n",
      "2813/2813 [==============================] - 1s 508us/step - loss: 0.1456 - accuracy: 0.0045\n",
      "Epoch 179/200\n",
      "2813/2813 [==============================] - 1s 514us/step - loss: 0.1438 - accuracy: 0.0045\n",
      "Epoch 180/200\n",
      "2813/2813 [==============================] - 1s 492us/step - loss: 0.1467 - accuracy: 0.0044\n",
      "Epoch 181/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1410 - accuracy: 0.0046\n",
      "Epoch 182/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1445 - accuracy: 0.0044\n",
      "Epoch 183/200\n",
      "2813/2813 [==============================] - 1s 510us/step - loss: 0.1430 - accuracy: 0.0045\n",
      "Epoch 184/200\n",
      "2813/2813 [==============================] - 1s 505us/step - loss: 0.1423 - accuracy: 0.0046\n",
      "Epoch 185/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1454 - accuracy: 0.0045\n",
      "Epoch 186/200\n",
      "2813/2813 [==============================] - 1s 498us/step - loss: 0.1418 - accuracy: 0.0045\n",
      "Epoch 187/200\n",
      "2813/2813 [==============================] - 1s 489us/step - loss: 0.1433 - accuracy: 0.0046\n",
      "Epoch 188/200\n",
      "2813/2813 [==============================] - 1s 494us/step - loss: 0.1437 - accuracy: 0.0046\n",
      "Epoch 189/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.1421 - accuracy: 0.0047\n",
      "Epoch 190/200\n",
      "2813/2813 [==============================] - 1s 488us/step - loss: 0.1414 - accuracy: 0.0046\n",
      "Epoch 191/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.1436 - accuracy: 0.0046\n",
      "Epoch 192/200\n",
      "2813/2813 [==============================] - 1s 499us/step - loss: 0.1513 - accuracy: 0.0045\n",
      "Epoch 193/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.1435 - accuracy: 0.0045\n",
      "Epoch 194/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1425 - accuracy: 0.0046\n",
      "Epoch 195/200\n",
      "2813/2813 [==============================] - 1s 493us/step - loss: 0.1428 - accuracy: 0.0046\n",
      "Epoch 196/200\n",
      "2813/2813 [==============================] - 1s 491us/step - loss: 0.1423 - accuracy: 0.0044\n",
      "Epoch 197/200\n",
      "2813/2813 [==============================] - 1s 495us/step - loss: 0.1425 - accuracy: 0.0046\n",
      "Epoch 198/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1451 - accuracy: 0.0046\n",
      "Epoch 199/200\n",
      "2813/2813 [==============================] - 1s 487us/step - loss: 0.1430 - accuracy: 0.0045\n",
      "Epoch 200/200\n",
      "2813/2813 [==============================] - 1s 496us/step - loss: 0.1411 - accuracy: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d3aba320>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the ANN on the training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 200, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 283us/step\n",
      "[[0.59278154 0.36724218]\n",
      " [1.04757905 1.6146172 ]\n",
      " [2.31513858 2.29060415]\n",
      " [1.77871871 1.66165015]\n",
      " [3.42865801 3.34930527]\n",
      " [5.38617277 5.337341  ]\n",
      " [4.45792341 4.79875161]\n",
      " [0.97207999 0.98978223]\n",
      " [0.45344448 0.44625305]\n",
      " [3.75552583 3.6178563 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.06%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred.flatten()>8) != (y_test>8))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "ann.save('ann_modelV1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all models and test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "modelL1 = load_model('ann_modelL1.h5')\n",
    "modelH1 = load_model('ann_modelH1.h5')\n",
    "modelV1 = load_model('ann_modelV1.h5')\n",
    "\n",
    "# load the scaler\n",
    "import pickle\n",
    "scalerL1 = pickle.load(open('scalerL1.pkl', 'rb'))\n",
    "scalerH1 = pickle.load(open('scalerH1.pkl', 'rb'))\n",
    "scalerV1 = pickle.load(open('scalerV1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_output_net(idx, params):\n",
    "\n",
    "    mass_1 = np.array(params['mass_1'][idx])\n",
    "    mass_2 = np.array(params['mass_2'][idx])\n",
    "    luminosity_distance = np.array(params['luminosity_distance'][idx])\n",
    "    theta_jn = np.array(params['theta_jn'][idx])\n",
    "    psi = np.array(params['psi'][idx])\n",
    "    geocent_time = np.array(params['geocent_time'][idx])\n",
    "    ra = np.array(params['ra'][idx])\n",
    "    dec = np.array(params['dec'][idx])\n",
    "    \n",
    "    detector_tensor = gwsnr.detector_tensor_list\n",
    "    snr_halfscaled = np.array(gwsnr.snr_partialsacaled_list)\n",
    "    ratio_arr = gwsnr.ratio_arr\n",
    "    mtot_arr = gwsnr.mtot_arr\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    len_ = len(detector_tensor)\n",
    "    mtot = mass_1 + mass_2\n",
    "    ratio = mass_2 / mass_1\n",
    "    # get array of antenna response\n",
    "    Fp, Fc = antenna_response_array(ra, dec, geocent_time, psi, detector_tensor)\n",
    "\n",
    "    Mc = ((mass_1 * mass_2) ** (3 / 5)) / ((mass_1 + mass_2) ** (1 / 5))\n",
    "    eta = mass_1 * mass_2/(mass_1 + mass_2)**2.\n",
    "    A1 = Mc ** (5.0 / 6.0)\n",
    "    ci_2 = np.cos(theta_jn) ** 2\n",
    "    ci_param = ((1 + np.cos(theta_jn) ** 2) / 2) ** 2\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    snr_half_ = np.zeros((len_,size))\n",
    "    d_eff = np.zeros((len_,size))\n",
    "\n",
    "    # loop over the detectors\n",
    "    for j in range(len_):\n",
    "        # loop over the parameter points\n",
    "        for i in range(size):\n",
    "            snr_half_coeff = snr_halfscaled[j]\n",
    "            snr_half_[j,i] = cubic_spline_interpolator2d(mtot[i], ratio[i], snr_half_coeff, mtot_arr, ratio_arr)\n",
    "            d_eff[j,i] =luminosity_distance[i] / np.sqrt(\n",
    "                    Fp[j,i]**2 * ci_param[i] + Fc[j,i]**2 * ci_2[i]\n",
    "                )\n",
    "\n",
    "    #amp0\n",
    "    amp0 =  A1 / d_eff\n",
    "\n",
    "    # get spin parameters\n",
    "    a_1 = np.array(params['a_1'][idx])\n",
    "    a_2 = np.array(params['a_2'][idx])\n",
    "    tilt_1 = np.array(params['tilt_1'][idx])\n",
    "    tilt_2 = np.array(params['tilt_2'][idx])\n",
    "    phi_12 = np.array(params['phi_12'][idx])\n",
    "    phi_jl = np.array(params['phi_jl'][idx])\n",
    "\n",
    "    # input data\n",
    "    L1 = np.vstack([snr_half_[0], amp0[0], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    H1 = np.vstack([snr_half_[1], amp0[1], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    V1 = np.vstack([snr_half_[2], amp0[2], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "\n",
    "    X = np.array([L1, H1, V1])\n",
    "\n",
    "    # output data\n",
    "    # get L1 snr for y train \n",
    "    y = np.sqrt(params['L1'][idx]**2 + params['H1'][idx]**2 + params['V1'][idx]**2)\n",
    "\n",
    "    return(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, Y_ = input_output_net(np.array([1000,1001,1002]), unlensed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_output_netANN(x_array):\n",
    "    x = scalerL1.transform(x_array[0])\n",
    "    yL1 = modelL1.predict(x)\n",
    "    x = scalerH1.transform(x_array[1])\n",
    "    yH1 = modelH1.predict(x)\n",
    "    x = scalerV1.transform(x_array[2])\n",
    "    yV1 = modelV1.predict(x)\n",
    "    y = np.sqrt(yL1**2 + yH1**2 + yV1**2)\n",
    "    return(x_array,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x179359a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x179359a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x29ebbfc70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x29ebbfc70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "_, snrANN =input_output_netANN(X_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8932943 , 33.87213955,  8.96373073])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8874264],\n",
       "       [33.258392 ],\n",
       "       [ 8.21772  ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snrANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ = len(unlensed_params['L1'])\n",
    "idx = np.arange(len_)\n",
    "# randomize the train set\n",
    "idx = np.random.permutation(idx)\n",
    "\n",
    "X_, Y_ = input_output_net(idx, unlensed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2414/2414 [==============================] - 1s 293us/step\n",
      "2414/2414 [==============================] - 1s 272us/step\n",
      "2414/2414 [==============================] - 1s 254us/step\n"
     ]
    }
   ],
   "source": [
    "_, snrANN =input_output_netANN(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.51425335, 22.90539453,  0.28999233, ...,  0.36397475,\n",
       "       14.99422556, 12.26884642])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.600124  ],\n",
       "       [26.772612  ],\n",
       "       [ 0.3860234 ],\n",
       "       ...,\n",
       "       [ 0.45380017],\n",
       "       [15.19776   ],\n",
       "       [12.837279  ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snrANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snrANN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43msnrANN\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      3\u001b[0m y_test \u001b[38;5;241m=\u001b[39m Y_\n\u001b[1;32m      4\u001b[0m hist_ \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m-\u001b[39my_test\n",
      "\u001b[0;31mNameError\u001b[0m: name 'snrANN' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = snrANN.flatten()\n",
    "y_test = Y_\n",
    "hist_ = y_pred-y_test\n",
    "print(len(hist_))\n",
    "idx = (y_test>4) & (y_test<100)\n",
    "hist_ = hist_[idx]\n",
    "#hist_ = hist_[abs(hist_)<5.]\n",
    "print(len(hist_))\n",
    "plt.figure(figsize=(4,4)) \n",
    "plt.hist(hist_, bins=100, histtype='step', density=True)\n",
    "plt.xlim(-5,5)\n",
    "plt.xlabel('snr_ml and snr_inner_product difference')\n",
    "plt.ylabel('pdf')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.16%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred>8) != (y_test>8))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38192   399]\n",
      " [  498 38151]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.988386846193682"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix((y_test>8), (y_pred>8))\n",
    "print(cm)\n",
    "accuracy_score((y_test>8), (y_pred>8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ler",
   "language": "python",
   "name": "ler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
