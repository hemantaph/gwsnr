{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psds not given. Choosing bilby's default psds\n",
      "npool:  4\n",
      "snr type:  ann\n",
      "waveform approximant:  IMRPhenomXPHM\n",
      "sampling frequency:  2048.0\n",
      "minimum frequency (fmin):  20.0\n",
      "mtot=mass1+mass2\n",
      "min(mtot):  2.0\n",
      "max(mtot) (with the given fmin=20.0): 184.98599853446768\n",
      "detectors:  None\n",
      "ANN method is selected.\n",
      "Please be patient while the interpolator is generated of partialscaledSNR for IMRPhenomD.\n",
      "Interpolator will be loaded for L1 detector from ./interpolator_pickle/L1/halfSNR_dict_1.pickle\n",
      "Interpolator will be loaded for H1 detector from ./interpolator_pickle/H1/halfSNR_dict_1.pickle\n",
      "Interpolator will be loaded for V1 detector from ./interpolator_pickle/V1/halfSNR_dict_1.pickle\n"
     ]
    }
   ],
   "source": [
    "from gwsnr import GWSNR\n",
    "gwsnr = GWSNR(snr_type='ann', waveform_approximant='IMRPhenomXPHM', pdet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pdet_L1': array([0, 0, 1, 0]),\n",
       " 'pdet_H1': array([0, 0, 0, 0]),\n",
       " 'pdet_V1': array([0, 0, 0, 0]),\n",
       " 'pdet_net': array([1, 0, 1, 0])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mass_1 = np.array([5, 10.,50.,200.])\n",
    "ratio = np.array([1, 0.8,0.5,0.2])\n",
    "luminosity_distance = np.array([1000, 2000, 3000, 4000])\n",
    "# gwsnr.snr_with_ann(mass_1=mass_1, mass_2=mass_1*ratio, luminosity_distance=luminosity_distance)\n",
    "gwsnr.snr(mass_1=mass_1, mass_2=mass_1*ratio, luminosity_distance=luminosity_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pdet_L1': array([0, 0, 1, 0]),\n",
       " 'pdet_H1': array([0, 0, 0, 0]),\n",
       " 'pdet_V1': array([0, 0, 0, 0]),\n",
       " 'pdet_net': array([1, 0, 1, 0])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snr_bilby = gwsnr.compute_bilby_snr(mass_1=mass_1, mass_2=mass_1*ratio, luminosity_distance=luminosity_distance)\n",
    "gwsnr.probability_of_detection(snr_dict=snr_bilby, type='bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "from ler.utils import append_json, load_json\n",
    "import numpy as np\n",
    "unlensed_params = load_json(\"jointnewL1.json\")\n",
    "# for key, value in unlensed_params.items():\n",
    "#     unlensed_params[key] = np.array(value)\n",
    "# randomize the data\n",
    "idx_shuffle = np.random.permutation(len(unlensed_params['L1']))\n",
    "for key, value in unlensed_params.items():\n",
    "    unlensed_params[key] = np.array(value)[idx_shuffle]\n",
    "#snr = np.array(unlensed_params['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psds not given. Choosing bilby's default psds\n",
      "npool:  4\n",
      "snr type:  ann\n",
      "waveform approximant:  IMRPhenomXPHM\n",
      "sampling frequency:  2048.0\n",
      "minimum frequency (fmin):  20.0\n",
      "mtot=mass1+mass2\n",
      "min(mtot):  2.0\n",
      "max(mtot) (with the given fmin=20.0): 184.98599853446768\n",
      "detectors:  None\n",
      "ANN method is selected.\n",
      "Please be patient while the interpolator is generated of partialscaledSNR for IMRPhenomD.\n",
      "Interpolator will be loaded for L1 detector from ./interpolator_pickle/L1/halfSNR_dict_1.pickle\n",
      "Interpolator will be loaded for H1 detector from ./interpolator_pickle/H1/halfSNR_dict_1.pickle\n",
      "Interpolator will be loaded for V1 detector from ./interpolator_pickle/V1/halfSNR_dict_1.pickle\n"
     ]
    }
   ],
   "source": [
    "# let's generate IMRPhenomD (spinless) interpolartor\n",
    "from gwsnr import GWSNR\n",
    "gwsnr = GWSNR(snr_type='ann', waveform_approximant='IMRPhenomXPHM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwsnr import antenna_response_array, cubic_spline_interpolator2d\n",
    "\n",
    "def input_output(idx, params):\n",
    "\n",
    "    mass_1 = np.array(params['mass_1'])[idx]\n",
    "    mass_2 = np.array(params['mass_2'])[idx]\n",
    "    luminosity_distance = np.array(params['luminosity_distance'])[idx]\n",
    "    theta_jn = np.array(params['theta_jn'])[idx]\n",
    "    psi = np.array(params['psi'])[idx]\n",
    "    geocent_time = np.array(params['geocent_time'])[idx]\n",
    "    ra = np.array(params['ra'])[idx]\n",
    "    dec = np.array(params['dec'])[idx]\n",
    "    \n",
    "    detector_tensor = gwsnr.detector_tensor_list\n",
    "    snr_halfscaled = np.array(gwsnr.snr_partialsacaled_list)\n",
    "    ratio_arr = gwsnr.ratio_arr\n",
    "    mtot_arr = gwsnr.mtot_arr\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    len_ = len(detector_tensor)\n",
    "    mtot = mass_1 + mass_2\n",
    "    ratio = mass_2 / mass_1\n",
    "    # get array of antenna response\n",
    "    Fp, Fc = antenna_response_array(ra, dec, geocent_time, psi, detector_tensor)\n",
    "\n",
    "    Mc = ((mass_1 * mass_2) ** (3 / 5)) / ((mass_1 + mass_2) ** (1 / 5))\n",
    "    eta = mass_1 * mass_2/(mass_1 + mass_2)**2.\n",
    "    A1 = Mc ** (5.0 / 6.0)\n",
    "    ci_2 = np.cos(theta_jn) ** 2\n",
    "    ci_param = ((1 + np.cos(theta_jn) ** 2) / 2) ** 2\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    snr_half_ = np.zeros((len_,size))\n",
    "    d_eff = np.zeros((len_,size))\n",
    "\n",
    "    # loop over the detectors\n",
    "    for j in range(len_):\n",
    "        # loop over the parameter points\n",
    "        for i in range(size):\n",
    "            snr_half_coeff = snr_halfscaled[j]\n",
    "            snr_half_[j,i] = cubic_spline_interpolator2d(mtot[i], ratio[i], snr_half_coeff, mtot_arr, ratio_arr)\n",
    "            d_eff[j,i] =luminosity_distance[i] / np.sqrt(\n",
    "                    Fp[j,i]**2 * ci_param[i] + Fc[j,i]**2 * ci_2[i]\n",
    "                )\n",
    "\n",
    "    #amp0\n",
    "    amp0 =  A1 / d_eff\n",
    "\n",
    "    # get spin parameters\n",
    "    a_1 = np.array(params['a_1'])[idx]\n",
    "    a_2 = np.array(params['a_2'])[idx]\n",
    "    tilt_1 = np.array(params['tilt_1'])[idx]\n",
    "    tilt_2 = np.array(params['tilt_2'])[idx]\n",
    "    phi_12 = np.array(params['phi_12'])[idx]\n",
    "    phi_jl = np.array(params['phi_jl'])[idx]\n",
    "\n",
    "    # input data\n",
    "    # X = np.vstack([L1, amp0, Mc, eta, theta_jn, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    XL1 = np.vstack([snr_half_[0], amp0[0], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    XH1 = np.vstack([snr_half_[1], amp0[1], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    XV1 = np.vstack([snr_half_[2], amp0[2], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "\n",
    "    # output data\n",
    "    # get L1 snr for y train \n",
    "    yL1 = params['L1'][idx]\n",
    "    yH1 = params['H1'][idx]\n",
    "    yV1 = params['V1'][idx]\n",
    "\n",
    "\n",
    "    return(XL1, XH1, XV1, yL1, yH1, yV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77240, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ = len(unlensed_params['L1'])\n",
    "idx = np.arange(len_)\n",
    "# randomize the train set\n",
    "idx = np.random.permutation(idx)\n",
    "XL1, XH1, XV1, yL1, yH1, yV1 = input_output(idx, unlensed_params)\n",
    "np.shape(XL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(y[y<100], bins=100)\n",
    "# plt.xlim(0,20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now back to ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XL1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mXL1\u001b[49m, yL1, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XL1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XL1, yL1, test_size = 0.1, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(X_train))\n",
    "# print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler\n",
    "import pickle\n",
    "pickle.dump(sc, open('scalerL1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "ann = tf.keras.models.Sequential() \n",
    "\n",
    "# adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=9, activation='relu'))\n",
    "# adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "# adding the third hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n",
    "# adding the output layer, absolute value of the snr\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2173/2173 [==============================] - 2s 519us/step - loss: 2461.3589 - accuracy: 0.0012\n",
      "Epoch 2/200\n",
      "2173/2173 [==============================] - 1s 488us/step - loss: 2432.8850 - accuracy: 0.0016\n",
      "Epoch 3/200\n",
      "2173/2173 [==============================] - 1s 515us/step - loss: 2427.8108 - accuracy: 0.0031\n",
      "Epoch 4/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 2424.9954 - accuracy: 0.0033\n",
      "Epoch 5/200\n",
      "2173/2173 [==============================] - 1s 484us/step - loss: 2422.7844 - accuracy: 0.0036\n",
      "Epoch 6/200\n",
      "2173/2173 [==============================] - 1s 510us/step - loss: 2420.0942 - accuracy: 0.0038\n",
      "Epoch 7/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 2418.3049 - accuracy: 0.0039\n",
      "Epoch 8/200\n",
      "2173/2173 [==============================] - 1s 485us/step - loss: 2416.9607 - accuracy: 0.0038\n",
      "Epoch 9/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 2415.7354 - accuracy: 0.0041\n",
      "Epoch 10/200\n",
      "2173/2173 [==============================] - 1s 482us/step - loss: 2414.2507 - accuracy: 0.0040\n",
      "Epoch 11/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 2413.1711 - accuracy: 0.0039\n",
      "Epoch 12/200\n",
      "2173/2173 [==============================] - 1s 480us/step - loss: 2412.1472 - accuracy: 0.0041\n",
      "Epoch 13/200\n",
      "2173/2173 [==============================] - 1s 512us/step - loss: 2411.2058 - accuracy: 0.0044\n",
      "Epoch 14/200\n",
      "2173/2173 [==============================] - 1s 515us/step - loss: 2409.8230 - accuracy: 0.0042\n",
      "Epoch 15/200\n",
      "2173/2173 [==============================] - 1s 475us/step - loss: 2408.8955 - accuracy: 0.0043\n",
      "Epoch 16/200\n",
      "2173/2173 [==============================] - 1s 509us/step - loss: 2407.8164 - accuracy: 0.0041\n",
      "Epoch 17/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 2406.9265 - accuracy: 0.0043\n",
      "Epoch 18/200\n",
      "2173/2173 [==============================] - 1s 490us/step - loss: 2406.2090 - accuracy: 0.0044\n",
      "Epoch 19/200\n",
      "2173/2173 [==============================] - 1s 510us/step - loss: 2405.1436 - accuracy: 0.0043\n",
      "Epoch 20/200\n",
      "2173/2173 [==============================] - 1s 475us/step - loss: 2404.4153 - accuracy: 0.0043\n",
      "Epoch 21/200\n",
      "2173/2173 [==============================] - 1s 515us/step - loss: 2403.7148 - accuracy: 0.0042\n",
      "Epoch 22/200\n",
      "2173/2173 [==============================] - 1s 484us/step - loss: 2402.9363 - accuracy: 0.0043\n",
      "Epoch 23/200\n",
      "2173/2173 [==============================] - 1s 510us/step - loss: 2401.8958 - accuracy: 0.0045\n",
      "Epoch 24/200\n",
      "2173/2173 [==============================] - 1s 515us/step - loss: 2401.0718 - accuracy: 0.0043\n",
      "Epoch 25/200\n",
      "2173/2173 [==============================] - 1s 484us/step - loss: 2400.4724 - accuracy: 0.0044\n",
      "Epoch 26/200\n",
      "2173/2173 [==============================] - 1s 511us/step - loss: 2399.7131 - accuracy: 0.0045\n",
      "Epoch 27/200\n",
      "2173/2173 [==============================] - 1s 481us/step - loss: 2399.0833 - accuracy: 0.0045\n",
      "Epoch 28/200\n",
      "2173/2173 [==============================] - 1s 522us/step - loss: 2398.2913 - accuracy: 0.0045\n",
      "Epoch 29/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 2397.6855 - accuracy: 0.0047\n",
      "Epoch 30/200\n",
      "2173/2173 [==============================] - 1s 475us/step - loss: 2397.1121 - accuracy: 0.0044\n",
      "Epoch 31/200\n",
      "2173/2173 [==============================] - 1s 520us/step - loss: 2396.4036 - accuracy: 0.0045\n",
      "Epoch 32/200\n",
      "2173/2173 [==============================] - 1s 513us/step - loss: 2395.6160 - accuracy: 0.0045\n",
      "Epoch 33/200\n",
      "2173/2173 [==============================] - 1s 479us/step - loss: 2394.8625 - accuracy: 0.0048\n",
      "Epoch 34/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 2394.3137 - accuracy: 0.0045\n",
      "Epoch 35/200\n",
      "2173/2173 [==============================] - 1s 476us/step - loss: 2393.7681 - accuracy: 0.0048\n",
      "Epoch 36/200\n",
      "2173/2173 [==============================] - 1s 511us/step - loss: 2392.9177 - accuracy: 0.0046\n",
      "Epoch 37/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 2392.2192 - accuracy: 0.0047\n",
      "Epoch 38/200\n",
      "2173/2173 [==============================] - 1s 476us/step - loss: 2391.6826 - accuracy: 0.0048\n",
      "Epoch 39/200\n",
      "2173/2173 [==============================] - 1s 509us/step - loss: 2390.9565 - accuracy: 0.0045\n",
      "Epoch 40/200\n",
      "2173/2173 [==============================] - 1s 476us/step - loss: 2390.3884 - accuracy: 0.0045\n",
      "Epoch 41/200\n",
      "2173/2173 [==============================] - 1s 517us/step - loss: 2389.6956 - accuracy: 0.0046\n",
      "Epoch 42/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 2389.1555 - accuracy: 0.0047\n",
      "Epoch 43/200\n",
      "2173/2173 [==============================] - 1s 477us/step - loss: 2388.5100 - accuracy: 0.0049\n",
      "Epoch 44/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 2388.0022 - accuracy: 0.0047\n",
      "Epoch 45/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 2387.1846 - accuracy: 0.0045\n",
      "Epoch 46/200\n",
      "2173/2173 [==============================] - 1s 485us/step - loss: 2386.6343 - accuracy: 0.0048\n",
      "Epoch 47/200\n",
      "2173/2173 [==============================] - 1s 511us/step - loss: 2386.1753 - accuracy: 0.0048\n",
      "Epoch 48/200\n",
      "2173/2173 [==============================] - 1s 477us/step - loss: 2385.4148 - accuracy: 0.0046\n",
      "Epoch 49/200\n",
      "2173/2173 [==============================] - 1s 519us/step - loss: 2384.8479 - accuracy: 0.0047\n",
      "Epoch 50/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 2384.4006 - accuracy: 0.0047\n",
      "Epoch 51/200\n",
      "2173/2173 [==============================] - 1s 479us/step - loss: 2383.8618 - accuracy: 0.0048\n",
      "Epoch 52/200\n",
      "2173/2173 [==============================] - 1s 518us/step - loss: 2383.2000 - accuracy: 0.0046\n",
      "Epoch 53/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 2382.5933 - accuracy: 0.0047\n",
      "Epoch 54/200\n",
      "2173/2173 [==============================] - 1s 483us/step - loss: 2382.1213 - accuracy: 0.0047\n",
      "Epoch 55/200\n",
      "2173/2173 [==============================] - 1s 521us/step - loss: 2381.6633 - accuracy: 0.0048\n",
      "Epoch 56/200\n",
      "2173/2173 [==============================] - 1s 481us/step - loss: 2381.0874 - accuracy: 0.0047\n",
      "Epoch 57/200\n",
      "2173/2173 [==============================] - 1s 517us/step - loss: 2380.5664 - accuracy: 0.0047\n",
      "Epoch 58/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 2379.9275 - accuracy: 0.0048\n",
      "Epoch 59/200\n",
      "2173/2173 [==============================] - 1s 489us/step - loss: 2379.4231 - accuracy: 0.0048\n",
      "Epoch 60/200\n",
      "2173/2173 [==============================] - 1s 519us/step - loss: 2378.9497 - accuracy: 0.0046\n",
      "Epoch 61/200\n",
      "2173/2173 [==============================] - 1s 517us/step - loss: 2378.3906 - accuracy: 0.0046\n",
      "Epoch 62/200\n",
      "2173/2173 [==============================] - 1s 482us/step - loss: 2377.8162 - accuracy: 0.0045\n",
      "Epoch 63/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 2377.3115 - accuracy: 0.0047\n",
      "Epoch 64/200\n",
      "2173/2173 [==============================] - 1s 518us/step - loss: 2376.8462 - accuracy: 0.0045\n",
      "Epoch 65/200\n",
      "2173/2173 [==============================] - 1s 484us/step - loss: 2376.4121 - accuracy: 0.0046\n",
      "Epoch 66/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 2375.8596 - accuracy: 0.0048\n",
      "Epoch 67/200\n",
      "2173/2173 [==============================] - 1s 514us/step - loss: 2375.3198 - accuracy: 0.0047\n",
      "Epoch 68/200\n",
      "2173/2173 [==============================] - 1s 487us/step - loss: 2374.8821 - accuracy: 0.0047\n",
      "Epoch 69/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 2374.3977 - accuracy: 0.0047\n",
      "Epoch 70/200\n",
      "2173/2173 [==============================] - 1s 492us/step - loss: 2373.8894 - accuracy: 0.0049\n",
      "Epoch 71/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 2373.4126 - accuracy: 0.0048\n",
      "Epoch 72/200\n",
      "2173/2173 [==============================] - 1s 522us/step - loss: 2372.9468 - accuracy: 0.0049\n",
      "Epoch 73/200\n",
      "2173/2173 [==============================] - 1s 485us/step - loss: 2372.3660 - accuracy: 0.0046\n",
      "Epoch 74/200\n",
      "2173/2173 [==============================] - 1s 524us/step - loss: 2371.8562 - accuracy: 0.0047\n",
      "Epoch 75/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 2371.3599 - accuracy: 0.0046\n",
      "Epoch 76/200\n",
      "2173/2173 [==============================] - 1s 485us/step - loss: 2370.8252 - accuracy: 0.0048\n",
      "Epoch 77/200\n",
      "2173/2173 [==============================] - 1s 520us/step - loss: 2370.4226 - accuracy: 0.0047\n",
      "Epoch 78/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 2369.9236 - accuracy: 0.0049\n",
      "Epoch 79/200\n",
      "2173/2173 [==============================] - 1s 491us/step - loss: 2369.4617 - accuracy: 0.0048\n",
      "Epoch 80/200\n",
      "2173/2173 [==============================] - 1s 521us/step - loss: 2368.9961 - accuracy: 0.0047\n",
      "Epoch 81/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 2368.5095 - accuracy: 0.0047\n",
      "Epoch 82/200\n",
      "2173/2173 [==============================] - 1s 491us/step - loss: 2368.0835 - accuracy: 0.0048\n",
      "Epoch 83/200\n",
      "2173/2173 [==============================] - 1s 558us/step - loss: 2367.4573 - accuracy: 0.0047\n",
      "Epoch 84/200\n",
      "2173/2173 [==============================] - 1s 502us/step - loss: 2367.0212 - accuracy: 0.0048\n",
      "Epoch 85/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2366.5759 - accuracy: 0.0049\n",
      "Epoch 86/200\n",
      "2173/2173 [==============================] - 1s 521us/step - loss: 2366.0676 - accuracy: 0.0048\n",
      "Epoch 87/200\n",
      "2173/2173 [==============================] - 1s 485us/step - loss: 2365.6438 - accuracy: 0.0046\n",
      "Epoch 88/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 2365.1133 - accuracy: 0.0049\n",
      "Epoch 89/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 2364.5959 - accuracy: 0.0047\n",
      "Epoch 90/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 2364.1028 - accuracy: 0.0048\n",
      "Epoch 91/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 2363.6694 - accuracy: 0.0047\n",
      "Epoch 92/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 2363.2388 - accuracy: 0.0044\n",
      "Epoch 93/200\n",
      "2173/2173 [==============================] - 1s 491us/step - loss: 2362.7993 - accuracy: 0.0049\n",
      "Epoch 94/200\n",
      "2173/2173 [==============================] - 1s 543us/step - loss: 2362.2554 - accuracy: 0.0048\n",
      "Epoch 95/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 2361.7712 - accuracy: 0.0045\n",
      "Epoch 96/200\n",
      "2173/2173 [==============================] - 1s 493us/step - loss: 2361.2644 - accuracy: 0.0049\n",
      "Epoch 97/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 2360.7515 - accuracy: 0.0048\n",
      "Epoch 98/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2360.3752 - accuracy: 0.0048\n",
      "Epoch 99/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 2359.8667 - accuracy: 0.0046\n",
      "Epoch 100/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 2359.3767 - accuracy: 0.0048\n",
      "Epoch 101/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2358.8401 - accuracy: 0.0048\n",
      "Epoch 102/200\n",
      "2173/2173 [==============================] - 1s 499us/step - loss: 2358.4536 - accuracy: 0.0049\n",
      "Epoch 103/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 2357.9612 - accuracy: 0.0049\n",
      "Epoch 104/200\n",
      "2173/2173 [==============================] - 1s 532us/step - loss: 2357.3286 - accuracy: 0.0046\n",
      "Epoch 105/200\n",
      "2173/2173 [==============================] - 1s 504us/step - loss: 2356.9138 - accuracy: 0.0047\n",
      "Epoch 106/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 2356.4448 - accuracy: 0.0047\n",
      "Epoch 107/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 2356.0105 - accuracy: 0.0045\n",
      "Epoch 108/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 2355.5540 - accuracy: 0.0047\n",
      "Epoch 109/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 2355.0789 - accuracy: 0.0046\n",
      "Epoch 110/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 2354.6321 - accuracy: 0.0048\n",
      "Epoch 111/200\n",
      "2173/2173 [==============================] - 1s 494us/step - loss: 2354.1658 - accuracy: 0.0044\n",
      "Epoch 112/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 2353.7588 - accuracy: 0.0043\n",
      "Epoch 113/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 2353.3701 - accuracy: 0.0045\n",
      "Epoch 114/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 2352.8918 - accuracy: 0.0048\n",
      "Epoch 115/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 2352.5073 - accuracy: 0.0047\n",
      "Epoch 116/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 2352.0723 - accuracy: 0.0049\n",
      "Epoch 117/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 2351.6050 - accuracy: 0.0047\n",
      "Epoch 118/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 2351.1177 - accuracy: 0.0048\n",
      "Epoch 119/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 2350.6792 - accuracy: 0.0047\n",
      "Epoch 120/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 2350.3657 - accuracy: 0.0047\n",
      "Epoch 121/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 2349.8660 - accuracy: 0.0047\n",
      "Epoch 122/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2349.4426 - accuracy: 0.0047\n",
      "Epoch 123/200\n",
      "2173/2173 [==============================] - 1s 500us/step - loss: 2348.9895 - accuracy: 0.0048\n",
      "Epoch 124/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 2348.6331 - accuracy: 0.0049\n",
      "Epoch 125/200\n",
      "2173/2173 [==============================] - 1s 549us/step - loss: 2348.1372 - accuracy: 0.0048\n",
      "Epoch 126/200\n",
      "2173/2173 [==============================] - 1s 499us/step - loss: 2347.6089 - accuracy: 0.0048\n",
      "Epoch 127/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 2347.2546 - accuracy: 0.0048\n",
      "Epoch 128/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2346.8538 - accuracy: 0.0049\n",
      "Epoch 129/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 2346.4302 - accuracy: 0.0050\n",
      "Epoch 130/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 2345.9941 - accuracy: 0.0046\n",
      "Epoch 131/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 2345.5999 - accuracy: 0.0050\n",
      "Epoch 132/200\n",
      "2173/2173 [==============================] - 1s 503us/step - loss: 2345.1926 - accuracy: 0.0048\n",
      "Epoch 133/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 2344.7095 - accuracy: 0.0047\n",
      "Epoch 134/200\n",
      "2173/2173 [==============================] - 1s 561us/step - loss: 2344.2534 - accuracy: 0.0048\n",
      "Epoch 135/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 2343.8467 - accuracy: 0.0049\n",
      "Epoch 136/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 2343.3804 - accuracy: 0.0049\n",
      "Epoch 137/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 2342.9907 - accuracy: 0.0048\n",
      "Epoch 138/200\n",
      "2173/2173 [==============================] - 1s 499us/step - loss: 2342.6172 - accuracy: 0.0048\n",
      "Epoch 139/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 2342.1274 - accuracy: 0.0048\n",
      "Epoch 140/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 2341.7388 - accuracy: 0.0050\n",
      "Epoch 141/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 2341.2893 - accuracy: 0.0049\n",
      "Epoch 142/200\n",
      "2173/2173 [==============================] - 1s 513us/step - loss: 2340.8726 - accuracy: 0.0050\n",
      "Epoch 143/200\n",
      "2173/2173 [==============================] - 1s 550us/step - loss: 2340.4766 - accuracy: 0.0050\n",
      "Epoch 144/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 2339.9636 - accuracy: 0.0048\n",
      "Epoch 145/200\n",
      "2173/2173 [==============================] - 1s 504us/step - loss: 2339.4866 - accuracy: 0.0048\n",
      "Epoch 146/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 2339.1233 - accuracy: 0.0050\n",
      "Epoch 147/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 2338.5776 - accuracy: 0.0050\n",
      "Epoch 148/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 2338.1328 - accuracy: 0.0050\n",
      "Epoch 149/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 2337.7815 - accuracy: 0.0050\n",
      "Epoch 150/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 2337.3743 - accuracy: 0.0050\n",
      "Epoch 151/200\n",
      "2173/2173 [==============================] - 1s 500us/step - loss: 2336.9412 - accuracy: 0.0050\n",
      "Epoch 152/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 2336.5254 - accuracy: 0.0051\n",
      "Epoch 153/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 2336.1951 - accuracy: 0.0050\n",
      "Epoch 154/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 2335.7627 - accuracy: 0.0049\n",
      "Epoch 155/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 2335.2610 - accuracy: 0.0049\n",
      "Epoch 156/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 2334.8735 - accuracy: 0.0050\n",
      "Epoch 157/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 2334.4690 - accuracy: 0.0048\n",
      "Epoch 158/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 2334.0776 - accuracy: 0.0050\n",
      "Epoch 159/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2333.5508 - accuracy: 0.0050\n",
      "Epoch 160/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 2333.1177 - accuracy: 0.0048\n",
      "Epoch 161/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 2332.7434 - accuracy: 0.0049\n",
      "Epoch 162/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 2332.3755 - accuracy: 0.0051\n",
      "Epoch 163/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 2331.9067 - accuracy: 0.0049\n",
      "Epoch 164/200\n",
      "2173/2173 [==============================] - 1s 494us/step - loss: 2331.5339 - accuracy: 0.0049\n",
      "Epoch 165/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 2331.0908 - accuracy: 0.0049\n",
      "Epoch 166/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 2330.6228 - accuracy: 0.0051\n",
      "Epoch 167/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 2330.2566 - accuracy: 0.0050\n",
      "Epoch 168/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 2329.7339 - accuracy: 0.0051\n",
      "Epoch 169/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 2329.2563 - accuracy: 0.0050\n",
      "Epoch 170/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 2328.8706 - accuracy: 0.0051\n",
      "Epoch 171/200\n",
      "2173/2173 [==============================] - 1s 543us/step - loss: 2328.4788 - accuracy: 0.0051\n",
      "Epoch 172/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 2328.1257 - accuracy: 0.0050\n",
      "Epoch 173/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 2327.7329 - accuracy: 0.0051\n",
      "Epoch 174/200\n",
      "2173/2173 [==============================] - 1s 503us/step - loss: 2327.1260 - accuracy: 0.0049\n",
      "Epoch 175/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2326.6624 - accuracy: 0.0049\n",
      "Epoch 176/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 2326.2861 - accuracy: 0.0052\n",
      "Epoch 177/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 2325.9177 - accuracy: 0.0051\n",
      "Epoch 178/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 2325.4736 - accuracy: 0.0051\n",
      "Epoch 179/200\n",
      "2173/2173 [==============================] - 1s 544us/step - loss: 2325.0955 - accuracy: 0.0050\n",
      "Epoch 180/200\n",
      "2173/2173 [==============================] - 1s 500us/step - loss: 2324.7207 - accuracy: 0.0050\n",
      "Epoch 181/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 2324.3013 - accuracy: 0.0050\n",
      "Epoch 182/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 2323.9436 - accuracy: 0.0051\n",
      "Epoch 183/200\n",
      "2173/2173 [==============================] - 1s 532us/step - loss: 2323.5020 - accuracy: 0.0050\n",
      "Epoch 184/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 2323.0366 - accuracy: 0.0050\n",
      "Epoch 185/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 2322.6299 - accuracy: 0.0051\n",
      "Epoch 186/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 2322.2654 - accuracy: 0.0051\n",
      "Epoch 187/200\n",
      "2173/2173 [==============================] - 1s 503us/step - loss: 2321.8379 - accuracy: 0.0052\n",
      "Epoch 188/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2321.4729 - accuracy: 0.0050\n",
      "Epoch 189/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 2321.2893 - accuracy: 0.0052\n",
      "Epoch 190/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 2320.6709 - accuracy: 0.0049\n",
      "Epoch 191/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 2320.1890 - accuracy: 0.0050\n",
      "Epoch 192/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 2319.8362 - accuracy: 0.0052\n",
      "Epoch 193/200\n",
      "2173/2173 [==============================] - 1s 532us/step - loss: 2319.3765 - accuracy: 0.0049\n",
      "Epoch 194/200\n",
      "2173/2173 [==============================] - 1s 509us/step - loss: 2319.0542 - accuracy: 0.0051\n",
      "Epoch 195/200\n",
      "2173/2173 [==============================] - 1s 547us/step - loss: 2318.5540 - accuracy: 0.0051\n",
      "Epoch 196/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 2318.2063 - accuracy: 0.0051\n",
      "Epoch 197/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 2317.7747 - accuracy: 0.0054\n",
      "Epoch 198/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 2317.3491 - accuracy: 0.0050\n",
      "Epoch 199/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 2316.8982 - accuracy: 0.0052\n",
      "Epoch 200/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 2316.4902 - accuracy: 0.0053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29e7cec20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the ANN on the training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 200, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 289us/step\n",
      "[[ 6.31955719  6.19221387]\n",
      " [ 7.10153484  7.52039616]\n",
      " [15.48692894 15.45474127]\n",
      " [ 3.82799244  3.78724652]\n",
      " [19.17204475 20.14008867]\n",
      " [ 3.09477806  3.13593516]\n",
      " [18.55880928 19.75293004]\n",
      " [ 6.14690828  6.19711101]\n",
      " [11.20474815 10.09325226]\n",
      " [ 2.21424007  2.38535518]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2.43%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred.flatten()>8) != (y_test>8))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "ann.save('ann_modelL1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XH1, yH1, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# save the scaler\n",
    "import pickle\n",
    "pickle.dump(sc, open('scalerH1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "ann = tf.keras.models.Sequential() \n",
    "\n",
    "# adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=9, activation='relu'))\n",
    "# adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "# adding the third hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n",
    "# adding the output layer, absolute value of the snr\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the ANN\n",
    "# loss = 'mean_squared_error'\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2173/2173 [==============================] - 1s 508us/step - loss: 1512.3213 - accuracy: 0.0013\n",
      "Epoch 2/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 1488.0007 - accuracy: 0.0019\n",
      "Epoch 3/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 1480.9833 - accuracy: 0.0031\n",
      "Epoch 4/200\n",
      "2173/2173 [==============================] - 1s 500us/step - loss: 1477.4314 - accuracy: 0.0039\n",
      "Epoch 5/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 1475.5961 - accuracy: 0.0040\n",
      "Epoch 6/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 1473.2994 - accuracy: 0.0039\n",
      "Epoch 7/200\n",
      "2173/2173 [==============================] - 1s 553us/step - loss: 1471.6851 - accuracy: 0.0038\n",
      "Epoch 8/200\n",
      "2173/2173 [==============================] - 1s 625us/step - loss: 1470.0814 - accuracy: 0.0039\n",
      "Epoch 9/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 1468.9038 - accuracy: 0.0041\n",
      "Epoch 10/200\n",
      "2173/2173 [==============================] - 1s 544us/step - loss: 1467.7701 - accuracy: 0.0043\n",
      "Epoch 11/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 1466.6289 - accuracy: 0.0041\n",
      "Epoch 12/200\n",
      "2173/2173 [==============================] - 1s 496us/step - loss: 1465.3860 - accuracy: 0.0046\n",
      "Epoch 13/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 1464.3934 - accuracy: 0.0045\n",
      "Epoch 14/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 1463.6116 - accuracy: 0.0045\n",
      "Epoch 15/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 1462.6481 - accuracy: 0.0042\n",
      "Epoch 16/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 1461.8470 - accuracy: 0.0043\n",
      "Epoch 17/200\n",
      "2173/2173 [==============================] - 1s 545us/step - loss: 1460.9487 - accuracy: 0.0043\n",
      "Epoch 18/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 1460.0016 - accuracy: 0.0041\n",
      "Epoch 19/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 1459.1367 - accuracy: 0.0040\n",
      "Epoch 20/200\n",
      "2173/2173 [==============================] - 1s 494us/step - loss: 1458.5236 - accuracy: 0.0043\n",
      "Epoch 21/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 1457.8892 - accuracy: 0.0042\n",
      "Epoch 22/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 1456.9764 - accuracy: 0.0040\n",
      "Epoch 23/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 1456.1699 - accuracy: 0.0042\n",
      "Epoch 24/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 1455.5533 - accuracy: 0.0041\n",
      "Epoch 25/200\n",
      "2173/2173 [==============================] - 1s 547us/step - loss: 1454.9038 - accuracy: 0.0040\n",
      "Epoch 26/200\n",
      "2173/2173 [==============================] - 1s 573us/step - loss: 1454.2272 - accuracy: 0.0044\n",
      "Epoch 27/200\n",
      "2173/2173 [==============================] - 1s 572us/step - loss: 1453.6024 - accuracy: 0.0043\n",
      "Epoch 28/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 1453.0009 - accuracy: 0.0042\n",
      "Epoch 29/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 1452.2830 - accuracy: 0.0043\n",
      "Epoch 30/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 1451.5803 - accuracy: 0.0043\n",
      "Epoch 31/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 1450.9279 - accuracy: 0.0043\n",
      "Epoch 32/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 1450.2732 - accuracy: 0.0044\n",
      "Epoch 33/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 1449.6533 - accuracy: 0.0043\n",
      "Epoch 34/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 1449.0668 - accuracy: 0.0042\n",
      "Epoch 35/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 1448.4723 - accuracy: 0.0045\n",
      "Epoch 36/200\n",
      "2173/2173 [==============================] - 1s 494us/step - loss: 1447.9176 - accuracy: 0.0044\n",
      "Epoch 37/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 1447.2515 - accuracy: 0.0045\n",
      "Epoch 38/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 1446.6997 - accuracy: 0.0045\n",
      "Epoch 39/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 1446.0924 - accuracy: 0.0044\n",
      "Epoch 40/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 1445.5448 - accuracy: 0.0043\n",
      "Epoch 41/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 1444.9204 - accuracy: 0.0045\n",
      "Epoch 42/200\n",
      "2173/2173 [==============================] - 1s 598us/step - loss: 1444.4022 - accuracy: 0.0045\n",
      "Epoch 43/200\n",
      "2173/2173 [==============================] - 1s 599us/step - loss: 1443.7363 - accuracy: 0.0046\n",
      "Epoch 44/200\n",
      "2173/2173 [==============================] - 1s 503us/step - loss: 1443.1301 - accuracy: 0.0046\n",
      "Epoch 45/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 1442.6056 - accuracy: 0.0045\n",
      "Epoch 46/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 1442.0823 - accuracy: 0.0046\n",
      "Epoch 47/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 1441.5739 - accuracy: 0.0045\n",
      "Epoch 48/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 1441.0271 - accuracy: 0.0047\n",
      "Epoch 49/200\n",
      "2173/2173 [==============================] - 1s 500us/step - loss: 1440.5299 - accuracy: 0.0045\n",
      "Epoch 50/200\n",
      "2173/2173 [==============================] - 1s 532us/step - loss: 1439.8977 - accuracy: 0.0047\n",
      "Epoch 51/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 1439.3673 - accuracy: 0.0048\n",
      "Epoch 52/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 1438.8849 - accuracy: 0.0046\n",
      "Epoch 53/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 1438.3813 - accuracy: 0.0045\n",
      "Epoch 54/200\n",
      "2173/2173 [==============================] - 1s 496us/step - loss: 1437.7737 - accuracy: 0.0045\n",
      "Epoch 55/200\n",
      "2173/2173 [==============================] - 1s 572us/step - loss: 1437.2128 - accuracy: 0.0047\n",
      "Epoch 56/200\n",
      "2173/2173 [==============================] - 1s 551us/step - loss: 1436.6821 - accuracy: 0.0046\n",
      "Epoch 57/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 1436.2068 - accuracy: 0.0048\n",
      "Epoch 58/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 1435.7445 - accuracy: 0.0047\n",
      "Epoch 59/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 1435.2397 - accuracy: 0.0045\n",
      "Epoch 60/200\n",
      "2173/2173 [==============================] - 1s 544us/step - loss: 1434.7231 - accuracy: 0.0045\n",
      "Epoch 61/200\n",
      "2173/2173 [==============================] - 1s 499us/step - loss: 1434.2052 - accuracy: 0.0045\n",
      "Epoch 62/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 1433.7365 - accuracy: 0.0047\n",
      "Epoch 63/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 1433.2136 - accuracy: 0.0047\n",
      "Epoch 64/200\n",
      "2173/2173 [==============================] - 1s 502us/step - loss: 1432.7567 - accuracy: 0.0047\n",
      "Epoch 65/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 1432.2616 - accuracy: 0.0049\n",
      "Epoch 66/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 1431.7729 - accuracy: 0.0047\n",
      "Epoch 67/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 1431.2407 - accuracy: 0.0046\n",
      "Epoch 68/200\n",
      "2173/2173 [==============================] - 1s 545us/step - loss: 1430.7281 - accuracy: 0.0047\n",
      "Epoch 69/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 1430.2131 - accuracy: 0.0048\n",
      "Epoch 70/200\n",
      "2173/2173 [==============================] - 1s 502us/step - loss: 1429.7190 - accuracy: 0.0048\n",
      "Epoch 71/200\n",
      "2173/2173 [==============================] - 1s 547us/step - loss: 1429.2697 - accuracy: 0.0047\n",
      "Epoch 72/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 1428.7869 - accuracy: 0.0047\n",
      "Epoch 73/200\n",
      "2173/2173 [==============================] - 1s 500us/step - loss: 1428.3535 - accuracy: 0.0048\n",
      "Epoch 74/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 1427.8936 - accuracy: 0.0048\n",
      "Epoch 75/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 1427.2437 - accuracy: 0.0049\n",
      "Epoch 76/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 1426.8051 - accuracy: 0.0051\n",
      "Epoch 77/200\n",
      "2173/2173 [==============================] - 1s 551us/step - loss: 1426.2720 - accuracy: 0.0050\n",
      "Epoch 78/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 1425.7170 - accuracy: 0.0045\n",
      "Epoch 79/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 1425.2950 - accuracy: 0.0048\n",
      "Epoch 80/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 1424.7976 - accuracy: 0.0048\n",
      "Epoch 81/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 1424.3407 - accuracy: 0.0049\n",
      "Epoch 82/200\n",
      "2173/2173 [==============================] - 1s 499us/step - loss: 1423.8993 - accuracy: 0.0048\n",
      "Epoch 83/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 1423.3743 - accuracy: 0.0050\n",
      "Epoch 84/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 1422.7949 - accuracy: 0.0048\n",
      "Epoch 85/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 1422.3506 - accuracy: 0.0051\n",
      "Epoch 86/200\n",
      "2173/2173 [==============================] - 1s 543us/step - loss: 1421.8976 - accuracy: 0.0049\n",
      "Epoch 87/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 1421.4684 - accuracy: 0.0049\n",
      "Epoch 88/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 1420.9482 - accuracy: 0.0051\n",
      "Epoch 89/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 1420.5266 - accuracy: 0.0049\n",
      "Epoch 90/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 1420.0120 - accuracy: 0.0050\n",
      "Epoch 91/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 1419.5635 - accuracy: 0.0048\n",
      "Epoch 92/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 1419.0554 - accuracy: 0.0049\n",
      "Epoch 93/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 1418.6294 - accuracy: 0.0049\n",
      "Epoch 94/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 1418.1791 - accuracy: 0.0050\n",
      "Epoch 95/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 1417.7408 - accuracy: 0.0049\n",
      "Epoch 96/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 1417.3105 - accuracy: 0.0051\n",
      "Epoch 97/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 1416.7205 - accuracy: 0.0050\n",
      "Epoch 98/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 1416.2902 - accuracy: 0.0050\n",
      "Epoch 99/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 1415.8169 - accuracy: 0.0049\n",
      "Epoch 100/200\n",
      "2173/2173 [==============================] - 1s 513us/step - loss: 1415.3470 - accuracy: 0.0049\n",
      "Epoch 101/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 1414.9475 - accuracy: 0.0050\n",
      "Epoch 102/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 1414.4633 - accuracy: 0.0051\n",
      "Epoch 103/200\n",
      "2173/2173 [==============================] - 1s 521us/step - loss: 1413.9606 - accuracy: 0.0050\n",
      "Epoch 104/200\n",
      "2173/2173 [==============================] - 1s 547us/step - loss: 1413.5623 - accuracy: 0.0052\n",
      "Epoch 105/200\n",
      "2173/2173 [==============================] - 1s 551us/step - loss: 1413.0388 - accuracy: 0.0051\n",
      "Epoch 106/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 1412.6226 - accuracy: 0.0050\n",
      "Epoch 107/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 1412.1370 - accuracy: 0.0049\n",
      "Epoch 108/200\n",
      "2173/2173 [==============================] - 1s 546us/step - loss: 1411.6013 - accuracy: 0.0050\n",
      "Epoch 109/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 1415.7834 - accuracy: 0.0051\n",
      "Epoch 110/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 1410.7352 - accuracy: 0.0051\n",
      "Epoch 111/200\n",
      "2173/2173 [==============================] - 1s 545us/step - loss: 1410.2623 - accuracy: 0.0049\n",
      "Epoch 112/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 1409.7854 - accuracy: 0.0050\n",
      "Epoch 113/200\n",
      "2173/2173 [==============================] - 1s 511us/step - loss: 1409.3483 - accuracy: 0.0050\n",
      "Epoch 114/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 1408.8777 - accuracy: 0.0049\n",
      "Epoch 115/200\n",
      "2173/2173 [==============================] - 1s 532us/step - loss: 1408.4655 - accuracy: 0.0050\n",
      "Epoch 116/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 1408.0874 - accuracy: 0.0050\n",
      "Epoch 117/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 1407.6635 - accuracy: 0.0051\n",
      "Epoch 118/200\n",
      "2173/2173 [==============================] - 1s 545us/step - loss: 1407.1897 - accuracy: 0.0049\n",
      "Epoch 119/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 1406.7507 - accuracy: 0.0050\n",
      "Epoch 120/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 1406.2781 - accuracy: 0.0051\n",
      "Epoch 121/200\n",
      "2173/2173 [==============================] - 1s 551us/step - loss: 1405.8596 - accuracy: 0.0051\n",
      "Epoch 122/200\n",
      "2173/2173 [==============================] - 1s 561us/step - loss: 1405.4690 - accuracy: 0.0051\n",
      "Epoch 123/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 1405.0630 - accuracy: 0.0049\n",
      "Epoch 124/200\n",
      "2173/2173 [==============================] - 1s 546us/step - loss: 1404.6224 - accuracy: 0.0050\n",
      "Epoch 125/200\n",
      "2173/2173 [==============================] - 1s 629us/step - loss: 1404.0511 - accuracy: 0.0050\n",
      "Epoch 126/200\n",
      "2173/2173 [==============================] - 1s 553us/step - loss: 1403.6439 - accuracy: 0.0051\n",
      "Epoch 127/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 1403.1506 - accuracy: 0.0049\n",
      "Epoch 128/200\n",
      "2173/2173 [==============================] - 1s 510us/step - loss: 1402.6698 - accuracy: 0.0051\n",
      "Epoch 129/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 1402.2566 - accuracy: 0.0052\n",
      "Epoch 130/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 1401.7532 - accuracy: 0.0051\n",
      "Epoch 131/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 1401.3563 - accuracy: 0.0052\n",
      "Epoch 132/200\n",
      "2173/2173 [==============================] - 1s 619us/step - loss: 1401.1875 - accuracy: 0.0054\n",
      "Epoch 133/200\n",
      "2173/2173 [==============================] - 1s 547us/step - loss: 1400.4962 - accuracy: 0.0052\n",
      "Epoch 134/200\n",
      "2173/2173 [==============================] - 1s 621us/step - loss: 1400.0225 - accuracy: 0.0052\n",
      "Epoch 135/200\n",
      "2173/2173 [==============================] - 1s 638us/step - loss: 1399.5707 - accuracy: 0.0051\n",
      "Epoch 136/200\n",
      "2173/2173 [==============================] - 1s 598us/step - loss: 1399.1355 - accuracy: 0.0054\n",
      "Epoch 137/200\n",
      "2173/2173 [==============================] - 1s 611us/step - loss: 1398.7410 - accuracy: 0.0053\n",
      "Epoch 138/200\n",
      "2173/2173 [==============================] - 1s 645us/step - loss: 1398.3542 - accuracy: 0.0052\n",
      "Epoch 139/200\n",
      "2173/2173 [==============================] - 1s 550us/step - loss: 1397.9027 - accuracy: 0.0050\n",
      "Epoch 140/200\n",
      "2173/2173 [==============================] - 1s 581us/step - loss: 1397.4800 - accuracy: 0.0052\n",
      "Epoch 141/200\n",
      "2173/2173 [==============================] - 1s 619us/step - loss: 1397.1038 - accuracy: 0.0049\n",
      "Epoch 142/200\n",
      "2173/2173 [==============================] - 1s 622us/step - loss: 1396.6597 - accuracy: 0.0052\n",
      "Epoch 143/200\n",
      "2173/2173 [==============================] - 1s 588us/step - loss: 1396.1891 - accuracy: 0.0051\n",
      "Epoch 144/200\n",
      "2173/2173 [==============================] - 1s 597us/step - loss: 1395.8044 - accuracy: 0.0053\n",
      "Epoch 145/200\n",
      "2173/2173 [==============================] - 1s 568us/step - loss: 1395.3873 - accuracy: 0.0052\n",
      "Epoch 146/200\n",
      "2173/2173 [==============================] - 1s 559us/step - loss: 1395.0155 - accuracy: 0.0051\n",
      "Epoch 147/200\n",
      "2173/2173 [==============================] - 1s 627us/step - loss: 1394.5985 - accuracy: 0.0055\n",
      "Epoch 148/200\n",
      "2173/2173 [==============================] - 1s 608us/step - loss: 1394.1683 - accuracy: 0.0053\n",
      "Epoch 149/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 1393.7306 - accuracy: 0.0052\n",
      "Epoch 150/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 1393.2640 - accuracy: 0.0052\n",
      "Epoch 151/200\n",
      "2173/2173 [==============================] - 1s 554us/step - loss: 1392.8345 - accuracy: 0.0051\n",
      "Epoch 152/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 1392.4240 - accuracy: 0.0051\n",
      "Epoch 153/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 1392.0425 - accuracy: 0.0051\n",
      "Epoch 154/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 1391.6405 - accuracy: 0.0051\n",
      "Epoch 155/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 1391.1455 - accuracy: 0.0051\n",
      "Epoch 156/200\n",
      "2173/2173 [==============================] - 1s 502us/step - loss: 1390.7124 - accuracy: 0.0053\n",
      "Epoch 157/200\n",
      "2173/2173 [==============================] - 1s 645us/step - loss: 1390.2371 - accuracy: 0.0053\n",
      "Epoch 158/200\n",
      "2173/2173 [==============================] - 1s 609us/step - loss: 1389.7716 - accuracy: 0.0050\n",
      "Epoch 159/200\n",
      "2173/2173 [==============================] - 1s 584us/step - loss: 1389.3525 - accuracy: 0.0053\n",
      "Epoch 160/200\n",
      "2173/2173 [==============================] - 1s 578us/step - loss: 1388.9132 - accuracy: 0.0052\n",
      "Epoch 161/200\n",
      "2173/2173 [==============================] - 1s 606us/step - loss: 1388.4908 - accuracy: 0.0054\n",
      "Epoch 162/200\n",
      "2173/2173 [==============================] - 1s 581us/step - loss: 1388.0819 - accuracy: 0.0053\n",
      "Epoch 163/200\n",
      "2173/2173 [==============================] - 1s 621us/step - loss: 1387.7317 - accuracy: 0.0053\n",
      "Epoch 164/200\n",
      "2173/2173 [==============================] - 1s 524us/step - loss: 1387.2740 - accuracy: 0.0054\n",
      "Epoch 165/200\n",
      "2173/2173 [==============================] - 1s 544us/step - loss: 1386.9089 - accuracy: 0.0055\n",
      "Epoch 166/200\n",
      "2173/2173 [==============================] - 1s 581us/step - loss: 1386.5339 - accuracy: 0.0054\n",
      "Epoch 167/200\n",
      "2173/2173 [==============================] - 1s 607us/step - loss: 1386.1318 - accuracy: 0.0054\n",
      "Epoch 168/200\n",
      "2173/2173 [==============================] - 1s 631us/step - loss: 1385.7609 - accuracy: 0.0054\n",
      "Epoch 169/200\n",
      "2173/2173 [==============================] - 1s 599us/step - loss: 1385.4390 - accuracy: 0.0052\n",
      "Epoch 170/200\n",
      "2173/2173 [==============================] - 1s 563us/step - loss: 1384.8894 - accuracy: 0.0053\n",
      "Epoch 171/200\n",
      "2173/2173 [==============================] - 1s 546us/step - loss: 1384.3890 - accuracy: 0.0054\n",
      "Epoch 172/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 1384.0255 - accuracy: 0.0052\n",
      "Epoch 173/200\n",
      "2173/2173 [==============================] - 1s 550us/step - loss: 1383.7019 - accuracy: 0.0052\n",
      "Epoch 174/200\n",
      "2173/2173 [==============================] - 1s 559us/step - loss: 1383.2834 - accuracy: 0.0051\n",
      "Epoch 175/200\n",
      "2173/2173 [==============================] - 1s 690us/step - loss: 1382.8246 - accuracy: 0.0055\n",
      "Epoch 176/200\n",
      "2173/2173 [==============================] - 1s 659us/step - loss: 1382.3568 - accuracy: 0.0055\n",
      "Epoch 177/200\n",
      "2173/2173 [==============================] - 1s 504us/step - loss: 1381.9648 - accuracy: 0.0056\n",
      "Epoch 178/200\n",
      "2173/2173 [==============================] - 1s 575us/step - loss: 1381.5637 - accuracy: 0.0054\n",
      "Epoch 179/200\n",
      "2173/2173 [==============================] - 1s 557us/step - loss: 1381.1720 - accuracy: 0.0052\n",
      "Epoch 180/200\n",
      "2173/2173 [==============================] - 1s 559us/step - loss: 1380.7812 - accuracy: 0.0054\n",
      "Epoch 181/200\n",
      "2173/2173 [==============================] - 1s 578us/step - loss: 1380.4144 - accuracy: 0.0053\n",
      "Epoch 182/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 1379.9093 - accuracy: 0.0054\n",
      "Epoch 183/200\n",
      "2173/2173 [==============================] - 1s 564us/step - loss: 1379.5205 - accuracy: 0.0055\n",
      "Epoch 184/200\n",
      "2173/2173 [==============================] - 1s 568us/step - loss: 1379.1000 - accuracy: 0.0054\n",
      "Epoch 185/200\n",
      "2173/2173 [==============================] - 1s 551us/step - loss: 1378.7400 - accuracy: 0.0055\n",
      "Epoch 186/200\n",
      "2173/2173 [==============================] - 1s 515us/step - loss: 1378.3063 - accuracy: 0.0057\n",
      "Epoch 187/200\n",
      "2173/2173 [==============================] - 1s 546us/step - loss: 1377.9200 - accuracy: 0.0056\n",
      "Epoch 188/200\n",
      "2173/2173 [==============================] - 1s 560us/step - loss: 1377.5187 - accuracy: 0.0054\n",
      "Epoch 189/200\n",
      "2173/2173 [==============================] - 1s 570us/step - loss: 1377.1890 - accuracy: 0.0055\n",
      "Epoch 190/200\n",
      "2173/2173 [==============================] - 1s 562us/step - loss: 1376.7505 - accuracy: 0.0056\n",
      "Epoch 191/200\n",
      "2173/2173 [==============================] - 1s 519us/step - loss: 1376.5067 - accuracy: 0.0055\n",
      "Epoch 192/200\n",
      "2173/2173 [==============================] - 1s 544us/step - loss: 1375.9456 - accuracy: 0.0056\n",
      "Epoch 193/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 1375.4824 - accuracy: 0.0055\n",
      "Epoch 194/200\n",
      "2173/2173 [==============================] - 1s 543us/step - loss: 1375.1151 - accuracy: 0.0053\n",
      "Epoch 195/200\n",
      "2173/2173 [==============================] - 1s 509us/step - loss: 1374.6692 - accuracy: 0.0057\n",
      "Epoch 196/200\n",
      "2173/2173 [==============================] - 1s 544us/step - loss: 1374.2960 - accuracy: 0.0055\n",
      "Epoch 197/200\n",
      "2173/2173 [==============================] - 2s 836us/step - loss: 1373.9304 - accuracy: 0.0057\n",
      "Epoch 198/200\n",
      "2173/2173 [==============================] - 2s 811us/step - loss: 1373.5912 - accuracy: 0.0056\n",
      "Epoch 199/200\n",
      "2173/2173 [==============================] - 1s 588us/step - loss: 1373.1265 - accuracy: 0.0056\n",
      "Epoch 200/200\n",
      "2173/2173 [==============================] - 1s 517us/step - loss: 1372.6117 - accuracy: 0.0056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29e6d4d90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the ANN on the training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 200, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 285us/step\n",
      "[[ 5.64273834  5.46883377]\n",
      " [10.9793005  10.70129808]\n",
      " [15.69080448 15.4378586 ]\n",
      " [ 3.41800499  3.47881364]\n",
      " [24.89271927 25.88531327]\n",
      " [ 2.58902168  2.74633207]\n",
      " [17.99505997 18.71973532]\n",
      " [ 5.50210953  5.63567116]\n",
      " [13.02195454 11.7723755 ]\n",
      " [ 1.86098099  2.33553513]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2.73%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred.flatten()>8) != (y_test>8))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "ann.save('ann_modelH1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(XV1, yV1, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# save the scaler\n",
    "import pickle\n",
    "pickle.dump(sc, open('scalerV1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "ann = tf.keras.models.Sequential() \n",
    "\n",
    "# adding the input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=9, activation='relu'))\n",
    "# adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "# adding the third hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n",
    "# adding the output layer, absolute value of the snr\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "#ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the ANN\n",
    "# loss = 'mean_squared_error'\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2173/2173 [==============================] - 1s 500us/step - loss: 753.1311 - accuracy: 0.0029\n",
      "Epoch 2/200\n",
      "2173/2173 [==============================] - 1s 521us/step - loss: 742.8470 - accuracy: 0.0037\n",
      "Epoch 3/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 740.0916 - accuracy: 0.0046\n",
      "Epoch 4/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 738.0026 - accuracy: 0.0049\n",
      "Epoch 5/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 736.1973 - accuracy: 0.0052\n",
      "Epoch 6/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 735.0300 - accuracy: 0.0053\n",
      "Epoch 7/200\n",
      "2173/2173 [==============================] - 1s 476us/step - loss: 733.9265 - accuracy: 0.0053\n",
      "Epoch 8/200\n",
      "2173/2173 [==============================] - 1s 512us/step - loss: 732.8179 - accuracy: 0.0054\n",
      "Epoch 9/200\n",
      "2173/2173 [==============================] - 1s 520us/step - loss: 731.8058 - accuracy: 0.0054\n",
      "Epoch 10/200\n",
      "2173/2173 [==============================] - 1s 479us/step - loss: 730.8839 - accuracy: 0.0055\n",
      "Epoch 11/200\n",
      "2173/2173 [==============================] - 1s 521us/step - loss: 730.0023 - accuracy: 0.0056\n",
      "Epoch 12/200\n",
      "2173/2173 [==============================] - 1s 482us/step - loss: 729.2567 - accuracy: 0.0058\n",
      "Epoch 13/200\n",
      "2173/2173 [==============================] - 1s 519us/step - loss: 728.5701 - accuracy: 0.0057\n",
      "Epoch 14/200\n",
      "2173/2173 [==============================] - 1s 516us/step - loss: 727.8809 - accuracy: 0.0058\n",
      "Epoch 15/200\n",
      "2173/2173 [==============================] - 1s 488us/step - loss: 727.1428 - accuracy: 0.0058\n",
      "Epoch 16/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 726.4619 - accuracy: 0.0058\n",
      "Epoch 17/200\n",
      "2173/2173 [==============================] - 1s 481us/step - loss: 725.7439 - accuracy: 0.0056\n",
      "Epoch 18/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 725.1536 - accuracy: 0.0055\n",
      "Epoch 19/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 724.5078 - accuracy: 0.0054\n",
      "Epoch 20/200\n",
      "2173/2173 [==============================] - 1s 511us/step - loss: 723.8959 - accuracy: 0.0056\n",
      "Epoch 21/200\n",
      "2173/2173 [==============================] - 1s 546us/step - loss: 723.3827 - accuracy: 0.0057\n",
      "Epoch 22/200\n",
      "2173/2173 [==============================] - 1s 548us/step - loss: 722.6508 - accuracy: 0.0056\n",
      "Epoch 23/200\n",
      "2173/2173 [==============================] - 1s 598us/step - loss: 722.0889 - accuracy: 0.0059\n",
      "Epoch 24/200\n",
      "2173/2173 [==============================] - 1s 487us/step - loss: 721.5554 - accuracy: 0.0061\n",
      "Epoch 25/200\n",
      "2173/2173 [==============================] - 1s 515us/step - loss: 720.9625 - accuracy: 0.0058\n",
      "Epoch 26/200\n",
      "2173/2173 [==============================] - 1s 510us/step - loss: 720.4849 - accuracy: 0.0058\n",
      "Epoch 27/200\n",
      "2173/2173 [==============================] - 1s 473us/step - loss: 719.8361 - accuracy: 0.0057\n",
      "Epoch 28/200\n",
      "2173/2173 [==============================] - 1s 510us/step - loss: 719.3585 - accuracy: 0.0058\n",
      "Epoch 29/200\n",
      "2173/2173 [==============================] - 1s 473us/step - loss: 718.8688 - accuracy: 0.0058\n",
      "Epoch 30/200\n",
      "2173/2173 [==============================] - 1s 504us/step - loss: 718.3472 - accuracy: 0.0058\n",
      "Epoch 31/200\n",
      "2173/2173 [==============================] - 1s 475us/step - loss: 717.7645 - accuracy: 0.0059\n",
      "Epoch 32/200\n",
      "2173/2173 [==============================] - 1s 514us/step - loss: 717.3267 - accuracy: 0.0059\n",
      "Epoch 33/200\n",
      "2173/2173 [==============================] - 1s 513us/step - loss: 716.8896 - accuracy: 0.0057\n",
      "Epoch 34/200\n",
      "2173/2173 [==============================] - 1s 480us/step - loss: 716.4272 - accuracy: 0.0058\n",
      "Epoch 35/200\n",
      "2173/2173 [==============================] - 1s 504us/step - loss: 715.9599 - accuracy: 0.0057\n",
      "Epoch 36/200\n",
      "2173/2173 [==============================] - 1s 472us/step - loss: 715.3890 - accuracy: 0.0058\n",
      "Epoch 37/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 714.9205 - accuracy: 0.0058\n",
      "Epoch 38/200\n",
      "2173/2173 [==============================] - 1s 477us/step - loss: 714.4970 - accuracy: 0.0058\n",
      "Epoch 39/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 714.0339 - accuracy: 0.0058\n",
      "Epoch 40/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 713.5350 - accuracy: 0.0058\n",
      "Epoch 41/200\n",
      "2173/2173 [==============================] - 1s 473us/step - loss: 713.0528 - accuracy: 0.0059\n",
      "Epoch 42/200\n",
      "2173/2173 [==============================] - 1s 517us/step - loss: 712.5768 - accuracy: 0.0058\n",
      "Epoch 43/200\n",
      "2173/2173 [==============================] - 1s 471us/step - loss: 712.1343 - accuracy: 0.0059\n",
      "Epoch 44/200\n",
      "2173/2173 [==============================] - 1s 513us/step - loss: 711.7097 - accuracy: 0.0058\n",
      "Epoch 45/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 711.2858 - accuracy: 0.0060\n",
      "Epoch 46/200\n",
      "2173/2173 [==============================] - 1s 472us/step - loss: 710.8138 - accuracy: 0.0057\n",
      "Epoch 47/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 710.3640 - accuracy: 0.0059\n",
      "Epoch 48/200\n",
      "2173/2173 [==============================] - 1s 503us/step - loss: 709.9502 - accuracy: 0.0060\n",
      "Epoch 49/200\n",
      "2173/2173 [==============================] - 1s 477us/step - loss: 709.4863 - accuracy: 0.0060\n",
      "Epoch 50/200\n",
      "2173/2173 [==============================] - 1s 508us/step - loss: 709.0505 - accuracy: 0.0060\n",
      "Epoch 51/200\n",
      "2173/2173 [==============================] - 1s 472us/step - loss: 708.5732 - accuracy: 0.0060\n",
      "Epoch 52/200\n",
      "2173/2173 [==============================] - 1s 511us/step - loss: 708.1451 - accuracy: 0.0059\n",
      "Epoch 53/200\n",
      "2173/2173 [==============================] - 1s 474us/step - loss: 707.7430 - accuracy: 0.0059\n",
      "Epoch 54/200\n",
      "2173/2173 [==============================] - 1s 524us/step - loss: 707.3292 - accuracy: 0.0061\n",
      "Epoch 55/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 706.8957 - accuracy: 0.0059\n",
      "Epoch 56/200\n",
      "2173/2173 [==============================] - 1s 477us/step - loss: 706.4498 - accuracy: 0.0060\n",
      "Epoch 57/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 706.0453 - accuracy: 0.0061\n",
      "Epoch 58/200\n",
      "2173/2173 [==============================] - 1s 474us/step - loss: 705.6600 - accuracy: 0.0062\n",
      "Epoch 59/200\n",
      "2173/2173 [==============================] - 1s 514us/step - loss: 705.2684 - accuracy: 0.0060\n",
      "Epoch 60/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 704.8474 - accuracy: 0.0060\n",
      "Epoch 61/200\n",
      "2173/2173 [==============================] - 1s 473us/step - loss: 704.4117 - accuracy: 0.0061\n",
      "Epoch 62/200\n",
      "2173/2173 [==============================] - 1s 507us/step - loss: 703.9976 - accuracy: 0.0059\n",
      "Epoch 63/200\n",
      "2173/2173 [==============================] - 1s 475us/step - loss: 703.5920 - accuracy: 0.0060\n",
      "Epoch 64/200\n",
      "2173/2173 [==============================] - 1s 552us/step - loss: 703.2001 - accuracy: 0.0061\n",
      "Epoch 65/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 702.7899 - accuracy: 0.0059\n",
      "Epoch 66/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 702.4202 - accuracy: 0.0061\n",
      "Epoch 67/200\n",
      "2173/2173 [==============================] - 1s 522us/step - loss: 702.0076 - accuracy: 0.0062\n",
      "Epoch 68/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 701.5952 - accuracy: 0.0061\n",
      "Epoch 69/200\n",
      "2173/2173 [==============================] - 1s 517us/step - loss: 701.1174 - accuracy: 0.0062\n",
      "Epoch 70/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 700.7092 - accuracy: 0.0062\n",
      "Epoch 71/200\n",
      "2173/2173 [==============================] - 1s 551us/step - loss: 700.3500 - accuracy: 0.0062\n",
      "Epoch 72/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 699.9736 - accuracy: 0.0062\n",
      "Epoch 73/200\n",
      "2173/2173 [==============================] - 1s 546us/step - loss: 699.5276 - accuracy: 0.0062\n",
      "Epoch 74/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 699.1517 - accuracy: 0.0063\n",
      "Epoch 75/200\n",
      "2173/2173 [==============================] - 1s 491us/step - loss: 698.7789 - accuracy: 0.0062\n",
      "Epoch 76/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 698.4136 - accuracy: 0.0063\n",
      "Epoch 77/200\n",
      "2173/2173 [==============================] - 1s 521us/step - loss: 698.0060 - accuracy: 0.0061\n",
      "Epoch 78/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 697.5667 - accuracy: 0.0063\n",
      "Epoch 79/200\n",
      "2173/2173 [==============================] - 1s 524us/step - loss: 697.2010 - accuracy: 0.0063\n",
      "Epoch 80/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 696.8488 - accuracy: 0.0064\n",
      "Epoch 81/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 696.4762 - accuracy: 0.0062\n",
      "Epoch 82/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 696.0615 - accuracy: 0.0061\n",
      "Epoch 83/200\n",
      "2173/2173 [==============================] - 1s 524us/step - loss: 695.6799 - accuracy: 0.0060\n",
      "Epoch 84/200\n",
      "2173/2173 [==============================] - 1s 493us/step - loss: 695.2568 - accuracy: 0.0062\n",
      "Epoch 85/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 694.8741 - accuracy: 0.0062\n",
      "Epoch 86/200\n",
      "2173/2173 [==============================] - 1s 519us/step - loss: 694.5472 - accuracy: 0.0064\n",
      "Epoch 87/200\n",
      "2173/2173 [==============================] - 1s 498us/step - loss: 694.1487 - accuracy: 0.0063\n",
      "Epoch 88/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 693.7742 - accuracy: 0.0062\n",
      "Epoch 89/200\n",
      "2173/2173 [==============================] - 1s 492us/step - loss: 693.3958 - accuracy: 0.0063\n",
      "Epoch 90/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 693.0405 - accuracy: 0.0062\n",
      "Epoch 91/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 692.6362 - accuracy: 0.0063\n",
      "Epoch 92/200\n",
      "2173/2173 [==============================] - 1s 491us/step - loss: 692.2452 - accuracy: 0.0065\n",
      "Epoch 93/200\n",
      "2173/2173 [==============================] - 1s 524us/step - loss: 691.8953 - accuracy: 0.0065\n",
      "Epoch 94/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 691.4996 - accuracy: 0.0064\n",
      "Epoch 95/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 691.0991 - accuracy: 0.0065\n",
      "Epoch 96/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 690.7214 - accuracy: 0.0064\n",
      "Epoch 97/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 690.4005 - accuracy: 0.0061\n",
      "Epoch 98/200\n",
      "2173/2173 [==============================] - 1s 488us/step - loss: 689.9811 - accuracy: 0.0064\n",
      "Epoch 99/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 689.5725 - accuracy: 0.0063\n",
      "Epoch 100/200\n",
      "2173/2173 [==============================] - 1s 517us/step - loss: 689.2191 - accuracy: 0.0064\n",
      "Epoch 101/200\n",
      "2173/2173 [==============================] - 1s 503us/step - loss: 688.8592 - accuracy: 0.0065\n",
      "Epoch 102/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 688.4629 - accuracy: 0.0064\n",
      "Epoch 103/200\n",
      "2173/2173 [==============================] - 1s 496us/step - loss: 688.0655 - accuracy: 0.0066\n",
      "Epoch 104/200\n",
      "2173/2173 [==============================] - 1s 524us/step - loss: 687.7229 - accuracy: 0.0064\n",
      "Epoch 105/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 687.3444 - accuracy: 0.0065\n",
      "Epoch 106/200\n",
      "2173/2173 [==============================] - 1s 499us/step - loss: 686.9481 - accuracy: 0.0065\n",
      "Epoch 107/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 686.6005 - accuracy: 0.0065\n",
      "Epoch 108/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 686.2250 - accuracy: 0.0065\n",
      "Epoch 109/200\n",
      "2173/2173 [==============================] - 1s 496us/step - loss: 685.8483 - accuracy: 0.0065\n",
      "Epoch 110/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 685.4685 - accuracy: 0.0066\n",
      "Epoch 111/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 685.0911 - accuracy: 0.0064\n",
      "Epoch 112/200\n",
      "2173/2173 [==============================] - 1s 488us/step - loss: 684.7696 - accuracy: 0.0066\n",
      "Epoch 113/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 684.3959 - accuracy: 0.0066\n",
      "Epoch 114/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 684.0157 - accuracy: 0.0063\n",
      "Epoch 115/200\n",
      "2173/2173 [==============================] - 1s 490us/step - loss: 683.6388 - accuracy: 0.0064\n",
      "Epoch 116/200\n",
      "2173/2173 [==============================] - 1s 523us/step - loss: 683.2635 - accuracy: 0.0064\n",
      "Epoch 117/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 682.9606 - accuracy: 0.0065\n",
      "Epoch 118/200\n",
      "2173/2173 [==============================] - 1s 491us/step - loss: 682.6035 - accuracy: 0.0063\n",
      "Epoch 119/200\n",
      "2173/2173 [==============================] - 1s 556us/step - loss: 682.1922 - accuracy: 0.0064\n",
      "Epoch 120/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 681.8792 - accuracy: 0.0062\n",
      "Epoch 121/200\n",
      "2173/2173 [==============================] - 1s 489us/step - loss: 681.4974 - accuracy: 0.0063\n",
      "Epoch 122/200\n",
      "2173/2173 [==============================] - 1s 533us/step - loss: 681.1235 - accuracy: 0.0065\n",
      "Epoch 123/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 680.7753 - accuracy: 0.0065\n",
      "Epoch 124/200\n",
      "2173/2173 [==============================] - 1s 500us/step - loss: 680.4301 - accuracy: 0.0060\n",
      "Epoch 125/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 680.0789 - accuracy: 0.0061\n",
      "Epoch 126/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 679.7062 - accuracy: 0.0063\n",
      "Epoch 127/200\n",
      "2173/2173 [==============================] - 1s 490us/step - loss: 679.3561 - accuracy: 0.0063\n",
      "Epoch 128/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 679.0002 - accuracy: 0.0062\n",
      "Epoch 129/200\n",
      "2173/2173 [==============================] - 1s 555us/step - loss: 678.6364 - accuracy: 0.0061\n",
      "Epoch 130/200\n",
      "2173/2173 [==============================] - 1s 544us/step - loss: 678.3127 - accuracy: 0.0061\n",
      "Epoch 131/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 677.9211 - accuracy: 0.0062\n",
      "Epoch 132/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 677.6691 - accuracy: 0.0060\n",
      "Epoch 133/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 677.2982 - accuracy: 0.0060\n",
      "Epoch 134/200\n",
      "2173/2173 [==============================] - 1s 496us/step - loss: 676.9480 - accuracy: 0.0061\n",
      "Epoch 135/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 676.6187 - accuracy: 0.0061\n",
      "Epoch 136/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 676.2238 - accuracy: 0.0063\n",
      "Epoch 137/200\n",
      "2173/2173 [==============================] - 1s 493us/step - loss: 675.9201 - accuracy: 0.0061\n",
      "Epoch 138/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 675.5280 - accuracy: 0.0062\n",
      "Epoch 139/200\n",
      "2173/2173 [==============================] - 1s 525us/step - loss: 675.1721 - accuracy: 0.0062\n",
      "Epoch 140/200\n",
      "2173/2173 [==============================] - 1s 496us/step - loss: 674.8253 - accuracy: 0.0061\n",
      "Epoch 141/200\n",
      "2173/2173 [==============================] - 1s 530us/step - loss: 674.4724 - accuracy: 0.0060\n",
      "Epoch 142/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 674.1547 - accuracy: 0.0061\n",
      "Epoch 143/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 673.7962 - accuracy: 0.0062\n",
      "Epoch 144/200\n",
      "2173/2173 [==============================] - 1s 526us/step - loss: 673.4227 - accuracy: 0.0060\n",
      "Epoch 145/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 673.0678 - accuracy: 0.0060\n",
      "Epoch 146/200\n",
      "2173/2173 [==============================] - 1s 527us/step - loss: 672.8414 - accuracy: 0.0063\n",
      "Epoch 147/200\n",
      "2173/2173 [==============================] - 1s 513us/step - loss: 672.4390 - accuracy: 0.0061\n",
      "Epoch 148/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 672.1133 - accuracy: 0.0062\n",
      "Epoch 149/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 671.7967 - accuracy: 0.0061\n",
      "Epoch 150/200\n",
      "2173/2173 [==============================] - 1s 491us/step - loss: 671.4125 - accuracy: 0.0062\n",
      "Epoch 151/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 671.0566 - accuracy: 0.0062\n",
      "Epoch 152/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 670.7339 - accuracy: 0.0063\n",
      "Epoch 153/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 670.4395 - accuracy: 0.0064\n",
      "Epoch 154/200\n",
      "2173/2173 [==============================] - 1s 497us/step - loss: 670.0619 - accuracy: 0.0063\n",
      "Epoch 155/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 669.7225 - accuracy: 0.0063\n",
      "Epoch 156/200\n",
      "2173/2173 [==============================] - 1s 545us/step - loss: 669.4058 - accuracy: 0.0065\n",
      "Epoch 157/200\n",
      "2173/2173 [==============================] - 1s 494us/step - loss: 669.1057 - accuracy: 0.0066\n",
      "Epoch 158/200\n",
      "2173/2173 [==============================] - 1s 539us/step - loss: 668.7182 - accuracy: 0.0064\n",
      "Epoch 159/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 668.4156 - accuracy: 0.0061\n",
      "Epoch 160/200\n",
      "2173/2173 [==============================] - 1s 504us/step - loss: 668.0715 - accuracy: 0.0062\n",
      "Epoch 161/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 667.7079 - accuracy: 0.0065\n",
      "Epoch 162/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 667.3869 - accuracy: 0.0064\n",
      "Epoch 163/200\n",
      "2173/2173 [==============================] - 1s 522us/step - loss: 667.0624 - accuracy: 0.0066\n",
      "Epoch 164/200\n",
      "2173/2173 [==============================] - 1s 503us/step - loss: 666.7761 - accuracy: 0.0065\n",
      "Epoch 165/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 666.4268 - accuracy: 0.0066\n",
      "Epoch 166/200\n",
      "2173/2173 [==============================] - 1s 509us/step - loss: 666.1013 - accuracy: 0.0065\n",
      "Epoch 167/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 665.7791 - accuracy: 0.0063\n",
      "Epoch 168/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 665.4471 - accuracy: 0.0065\n",
      "Epoch 169/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 665.0994 - accuracy: 0.0066\n",
      "Epoch 170/200\n",
      "2173/2173 [==============================] - 1s 502us/step - loss: 664.7804 - accuracy: 0.0064\n",
      "Epoch 171/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 664.6228 - accuracy: 0.0065\n",
      "Epoch 172/200\n",
      "2173/2173 [==============================] - 1s 532us/step - loss: 664.1340 - accuracy: 0.0066\n",
      "Epoch 173/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 663.7917 - accuracy: 0.0067\n",
      "Epoch 174/200\n",
      "2173/2173 [==============================] - 1s 528us/step - loss: 663.4168 - accuracy: 0.0066\n",
      "Epoch 175/200\n",
      "2173/2173 [==============================] - 1s 545us/step - loss: 663.1731 - accuracy: 0.0065\n",
      "Epoch 176/200\n",
      "2173/2173 [==============================] - 1s 541us/step - loss: 662.8698 - accuracy: 0.0065\n",
      "Epoch 177/200\n",
      "2173/2173 [==============================] - 1s 499us/step - loss: 662.5347 - accuracy: 0.0067\n",
      "Epoch 178/200\n",
      "2173/2173 [==============================] - 1s 542us/step - loss: 662.3095 - accuracy: 0.0065\n",
      "Epoch 179/200\n",
      "2173/2173 [==============================] - 1s 529us/step - loss: 661.8618 - accuracy: 0.0065\n",
      "Epoch 180/200\n",
      "2173/2173 [==============================] - 1s 501us/step - loss: 661.5681 - accuracy: 0.0065\n",
      "Epoch 181/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 661.2233 - accuracy: 0.0067\n",
      "Epoch 182/200\n",
      "2173/2173 [==============================] - 1s 532us/step - loss: 660.9514 - accuracy: 0.0065\n",
      "Epoch 183/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 660.6395 - accuracy: 0.0065\n",
      "Epoch 184/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 660.2799 - accuracy: 0.0067\n",
      "Epoch 185/200\n",
      "2173/2173 [==============================] - 1s 536us/step - loss: 659.9996 - accuracy: 0.0066\n",
      "Epoch 186/200\n",
      "2173/2173 [==============================] - 1s 534us/step - loss: 659.6581 - accuracy: 0.0066\n",
      "Epoch 187/200\n",
      "2173/2173 [==============================] - 1s 495us/step - loss: 659.3646 - accuracy: 0.0067\n",
      "Epoch 188/200\n",
      "2173/2173 [==============================] - 1s 547us/step - loss: 659.0394 - accuracy: 0.0066\n",
      "Epoch 189/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 658.7344 - accuracy: 0.0067\n",
      "Epoch 190/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 658.4243 - accuracy: 0.0067\n",
      "Epoch 191/200\n",
      "2173/2173 [==============================] - 1s 537us/step - loss: 658.1221 - accuracy: 0.0068\n",
      "Epoch 192/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 657.7801 - accuracy: 0.0068\n",
      "Epoch 193/200\n",
      "2173/2173 [==============================] - 1s 545us/step - loss: 657.4911 - accuracy: 0.0068\n",
      "Epoch 194/200\n",
      "2173/2173 [==============================] - 1s 506us/step - loss: 657.1683 - accuracy: 0.0067\n",
      "Epoch 195/200\n",
      "2173/2173 [==============================] - 1s 531us/step - loss: 656.8772 - accuracy: 0.0067\n",
      "Epoch 196/200\n",
      "2173/2173 [==============================] - 1s 540us/step - loss: 656.5458 - accuracy: 0.0066\n",
      "Epoch 197/200\n",
      "2173/2173 [==============================] - 1s 505us/step - loss: 656.2363 - accuracy: 0.0067\n",
      "Epoch 198/200\n",
      "2173/2173 [==============================] - 1s 535us/step - loss: 655.8961 - accuracy: 0.0068\n",
      "Epoch 199/200\n",
      "2173/2173 [==============================] - 1s 538us/step - loss: 655.6075 - accuracy: 0.0067\n",
      "Epoch 200/200\n",
      "2173/2173 [==============================] - 1s 543us/step - loss: 655.2793 - accuracy: 0.0068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c67ee380>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the ANN on the training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 200, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 289us/step\n",
      "[[ 4.70164871  4.60294629]\n",
      " [ 7.08188534  7.20077038]\n",
      " [ 9.95392418  9.69107048]\n",
      " [ 1.41868019  1.464477  ]\n",
      " [19.21713829 18.97517837]\n",
      " [ 1.91390228  2.08792123]\n",
      " [20.99995422 21.31686381]\n",
      " [ 7.28342438  7.24676503]\n",
      " [12.31026649  9.60957984]\n",
      " [ 0.60850334  0.76886953]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.40%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred.flatten()>8) != (y_test>8))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "ann.save('ann_modelV1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all models and test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "modelL1 = load_model('ann_modelL1.h5')\n",
    "modelH1 = load_model('ann_modelH1.h5')\n",
    "modelV1 = load_model('ann_modelV1.h5')\n",
    "\n",
    "# load the scaler\n",
    "import pickle\n",
    "scalerL1 = pickle.load(open('scalerL1.pkl', 'rb'))\n",
    "scalerH1 = pickle.load(open('scalerH1.pkl', 'rb'))\n",
    "scalerV1 = pickle.load(open('scalerV1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_output_net(idx, params):\n",
    "\n",
    "    mass_1 = np.array(params['mass_1'][idx])\n",
    "    mass_2 = np.array(params['mass_2'][idx])\n",
    "    luminosity_distance = np.array(params['luminosity_distance'][idx])\n",
    "    theta_jn = np.array(params['theta_jn'][idx])\n",
    "    psi = np.array(params['psi'][idx])\n",
    "    geocent_time = np.array(params['geocent_time'][idx])\n",
    "    ra = np.array(params['ra'][idx])\n",
    "    dec = np.array(params['dec'][idx])\n",
    "    \n",
    "    detector_tensor = gwsnr.detector_tensor_list\n",
    "    snr_halfscaled = np.array(gwsnr.snr_partialsacaled_list)\n",
    "    ratio_arr = gwsnr.ratio_arr\n",
    "    mtot_arr = gwsnr.mtot_arr\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    len_ = len(detector_tensor)\n",
    "    mtot = mass_1 + mass_2\n",
    "    ratio = mass_2 / mass_1\n",
    "    # get array of antenna response\n",
    "    Fp, Fc = antenna_response_array(ra, dec, geocent_time, psi, detector_tensor)\n",
    "\n",
    "    Mc = ((mass_1 * mass_2) ** (3 / 5)) / ((mass_1 + mass_2) ** (1 / 5))\n",
    "    eta = mass_1 * mass_2/(mass_1 + mass_2)**2.\n",
    "    A1 = Mc ** (5.0 / 6.0)\n",
    "    ci_2 = np.cos(theta_jn) ** 2\n",
    "    ci_param = ((1 + np.cos(theta_jn) ** 2) / 2) ** 2\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    snr_half_ = np.zeros((len_,size))\n",
    "    d_eff = np.zeros((len_,size))\n",
    "\n",
    "    # loop over the detectors\n",
    "    for j in range(len_):\n",
    "        # loop over the parameter points\n",
    "        for i in range(size):\n",
    "            snr_half_coeff = snr_halfscaled[j]\n",
    "            snr_half_[j,i] = cubic_spline_interpolator2d(mtot[i], ratio[i], snr_half_coeff, mtot_arr, ratio_arr)\n",
    "            d_eff[j,i] =luminosity_distance[i] / np.sqrt(\n",
    "                    Fp[j,i]**2 * ci_param[i] + Fc[j,i]**2 * ci_2[i]\n",
    "                )\n",
    "\n",
    "    #amp0\n",
    "    amp0 =  A1 / d_eff\n",
    "\n",
    "    # get spin parameters\n",
    "    a_1 = np.array(params['a_1'][idx])\n",
    "    a_2 = np.array(params['a_2'][idx])\n",
    "    tilt_1 = np.array(params['tilt_1'][idx])\n",
    "    tilt_2 = np.array(params['tilt_2'][idx])\n",
    "    phi_12 = np.array(params['phi_12'][idx])\n",
    "    phi_jl = np.array(params['phi_jl'][idx])\n",
    "\n",
    "    # input data\n",
    "    L1 = np.vstack([snr_half_[0], amp0[0], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    H1 = np.vstack([snr_half_[1], amp0[1], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "    V1 = np.vstack([snr_half_[2], amp0[2], eta, a_1, a_2, tilt_1, tilt_2, phi_12, phi_jl]).T\n",
    "\n",
    "    X = np.array([L1, H1, V1])\n",
    "\n",
    "    # output data\n",
    "    # get L1 snr for y train \n",
    "    y = np.sqrt(params['L1'][idx]**2 + params['H1'][idx]**2 + params['V1'][idx]**2)\n",
    "\n",
    "    return(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, Y_ = input_output_net(np.array([1000,1001,1002]), unlensed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_output_netANN(x_array):\n",
    "    x = scalerL1.transform(x_array[0])\n",
    "    yL1 = modelL1.predict(x)\n",
    "    x = scalerH1.transform(x_array[1])\n",
    "    yH1 = modelH1.predict(x)\n",
    "    x = scalerV1.transform(x_array[2])\n",
    "    yV1 = modelV1.predict(x)\n",
    "    y = np.sqrt(yL1**2 + yH1**2 + yV1**2)\n",
    "    return(x_array,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x179359a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x179359a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x29ebbfc70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x29ebbfc70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "_, snrANN =input_output_netANN(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8932943 , 33.87213955,  8.96373073])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8874264],\n",
       "       [33.258392 ],\n",
       "       [ 8.21772  ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snrANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ = len(unlensed_params['L1'])\n",
    "idx = np.arange(len_)\n",
    "# randomize the train set\n",
    "idx = np.random.permutation(idx)\n",
    "\n",
    "X_, Y_ = input_output_net(idx, unlensed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2414/2414 [==============================] - 1s 293us/step\n",
      "2414/2414 [==============================] - 1s 272us/step\n",
      "2414/2414 [==============================] - 1s 254us/step\n"
     ]
    }
   ],
   "source": [
    "_, snrANN =input_output_netANN(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.51425335, 22.90539453,  0.28999233, ...,  0.36397475,\n",
       "       14.99422556, 12.26884642])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.600124  ],\n",
       "       [26.772612  ],\n",
       "       [ 0.3860234 ],\n",
       "       ...,\n",
       "       [ 0.45380017],\n",
       "       [15.19776   ],\n",
       "       [12.837279  ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snrANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77240\n",
      "48211\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_pred = snrANN.flatten()\n",
    "y_test = Y_\n",
    "hist_ = y_pred-y_test\n",
    "print(len(hist_))\n",
    "idx = (y_test>4) & (y_test<100)\n",
    "hist_ = hist_[idx]\n",
    "#hist_ = hist_[abs(hist_)<5.]\n",
    "print(len(hist_))\n",
    "plt.figure(figsize=(4,4)) \n",
    "plt.hist(hist_, bins=100, histtype='step', density=True)\n",
    "plt.xlim(-5,5)\n",
    "plt.xlabel('snr_ml and snr_inner_product difference')\n",
    "plt.ylabel('pdf')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.16%\n"
     ]
    }
   ],
   "source": [
    "len1 = len(y_pred)\n",
    "len2 = np.sum((y_pred>8) != (y_test>8))\n",
    "error = len2/len1*100\n",
    "print(f\"Error: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38192   399]\n",
      " [  498 38151]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.988386846193682"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix((y_test>8), (y_pred>8))\n",
    "print(cm)\n",
    "accuracy_score((y_test>8), (y_pred>8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ler",
   "language": "python",
   "name": "ler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
