{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gwsnr import GWSNR\n",
    "from gwsnr import antenna_response_array, cubic_spline_interpolator2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psds not given. Choosing bilby's default psds\n",
      "Intel processor has trouble allocating memory when the data is huge. So, by default for IMRPhenomXPHM, duration_max = 64.0. Otherwise, set to some max value like duration_max = 600.0 (10 mins)\n",
      "Interpolator will be loaded for L1 detector from ./interpolator_pickle/L1/partialSNR_dict_0.pickle\n",
      "Interpolator will be loaded for H1 detector from ./interpolator_pickle/H1/partialSNR_dict_0.pickle\n",
      "Interpolator will be loaded for V1 detector from ./interpolator_pickle/V1/partialSNR_dict_0.pickle\n",
      "\n",
      "Chosen GWSNR initialization parameters:\n",
      "\n",
      "npool:  4\n",
      "snr type:  ann\n",
      "waveform approximant:  IMRPhenomXPHM\n",
      "sampling frequency:  2048.0\n",
      "minimum frequency (fmin):  20.0\n",
      "mtot=mass1+mass2\n",
      "min(mtot):  2.0\n",
      "max(mtot) (with the given fmin=20.0): 184.98599853446768\n",
      "detectors:  ['L1', 'H1', 'V1']\n",
      "psds:  [PowerSpectralDensity(psd_file='None', asd_file='/Users/phurailatpamhemantakumar/anaconda3/envs/lertest/lib/python3.10/site-packages/bilby/gw/detector/noise_curves/aLIGO_O4_high_asd.txt'), PowerSpectralDensity(psd_file='None', asd_file='/Users/phurailatpamhemantakumar/anaconda3/envs/lertest/lib/python3.10/site-packages/bilby/gw/detector/noise_curves/aLIGO_O4_high_asd.txt'), PowerSpectralDensity(psd_file='None', asd_file='/Users/phurailatpamhemantakumar/anaconda3/envs/lertest/lib/python3.10/site-packages/bilby/gw/detector/noise_curves/AdV_asd.txt')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phurailatpamhemantakumar/anaconda3/envs/lertest/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.5.0 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gwsnr = GWSNR(npool=4, waveform_approximant=\"IMRPhenomXPHM\", duration_max=None, snr_type=\"ann\", pdet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': array([6.64308357, 5.40514946, 9.97871399, 0.        ]),\n",
       " 'H1': array([3.85338736, 3.0341351 , 6.47034931, 0.        ]),\n",
       " 'V1': array([2.27271104, 1.77481127, 3.16478086, 0.        ]),\n",
       " 'optimal_snr_net': array([ 8.00901798,  6.447602  , 12.30674575,  0.        ])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the GW parameters\n",
    "mass_1 = np.array([5, 10.,50.,200.])\n",
    "ratio = np.array([1, 0.8,0.5,0.2])\n",
    "luminosity_distance = np.array([1000, 2000, 3000, 4000])\n",
    "a_1 = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "a_2 = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "tilt_1 = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "tilt_2 = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "phi_12 = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "phi_jl = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "# pdet calculation with ANN\n",
    "gwsnr.snr(mass_1=mass_1, mass_2=mass_1*ratio, luminosity_distance=luminosity_distance, a_1=a_1, a_2=a_2, tilt_1=tilt_1, tilt_2=tilt_2, phi_12=phi_12, phi_jl=phi_jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwsnr.utils import (\n",
    "    dealing_with_psds,\n",
    "    interpolator_check,\n",
    "    load_json,\n",
    "    load_pickle,\n",
    "    save_pickle,\n",
    "    save_json,\n",
    "    load_ann_h5_from_module,\n",
    "    load_ann_h5,\n",
    "    load_pickle_from_module,\n",
    "    load_json_from_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# load the ANN models\n",
    "modelL1 = load_ann_h5('../ann_modelL1_final.h5')\n",
    "modelH1 = load_ann_h5('../ann_modelH1_final.h5')\n",
    "modelV1 = load_ann_h5('../ann_modelV1_final.h5')\n",
    "\n",
    "# load the feature scaler\n",
    "scalerL1 = load_pickle('../scalerL1_final.pkl')\n",
    "scalerH1 = load_pickle('../scalerH1_final.pkl')\n",
    "scalerV1 = load_pickle('../scalerV1_final.pkl')\n",
    "\n",
    "# load the correction slope and intercept\n",
    "correctionL1 = load_json('../error_adjustmentL1_final.json')\n",
    "correctionH1 = load_json('../error_adjustmentH1_final.json')\n",
    "correctionV1 = load_json('../error_adjustmentV1_final.json')\n",
    "aL1 = correctionL1['slope']\n",
    "bL1 = correctionL1['intercept']\n",
    "aH1 = correctionH1['slope']\n",
    "bH1 = correctionH1['intercept']\n",
    "aV1 = correctionV1['slope']\n",
    "bV1 = correctionV1['intercept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlensed_params = load_json(\"../ler_data/unlensed_param_testing.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_output(idx, params):\n",
    "    \"\"\"\n",
    "        Function to generate input and output data for the neural network\n",
    "\n",
    "        Parameters:\n",
    "        idx: index of the parameter points\n",
    "        params: dictionary of parameter points\n",
    "            params.keys() = ['mass_1', 'mass_2', 'luminosity_distance', 'theta_jn', 'psi', 'geocent_time', 'ra', 'dec', 'a_1', 'a_2', 'tilt_1', 'tilt_2', 'L1']\n",
    "\n",
    "        Returns:\n",
    "        X: input data, [snr_half_[0], amp0[0], eta, chi_eff, theta_jn]\n",
    "        y: output data, [L1]\n",
    "    \"\"\"\n",
    "\n",
    "    mass_1 = np.array(params['mass_1'])[idx]\n",
    "    mass_2 = np.array(params['mass_2'])[idx]\n",
    "    luminosity_distance = np.array(params['luminosity_distance'])[idx]\n",
    "    theta_jn = np.array(params['theta_jn'])[idx]\n",
    "    psi = np.array(params['psi'])[idx]\n",
    "    geocent_time = np.array(params['geocent_time'])[idx]\n",
    "    ra = np.array(params['ra'])[idx]\n",
    "    dec = np.array(params['dec'])[idx]\n",
    "    \n",
    "    detector_tensor = gwsnr.detector_tensor_list\n",
    "    snr_halfscaled = np.array(gwsnr.snr_partialsacaled_list)\n",
    "    ratio_arr = gwsnr.ratio_arr\n",
    "    mtot_arr = gwsnr.mtot_arr\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    len_ = len(detector_tensor)\n",
    "    mtot = mass_1 + mass_2\n",
    "    ratio = mass_2 / mass_1\n",
    "    # get array of antenna response\n",
    "    Fp, Fc = antenna_response_array(ra, dec, geocent_time, psi, detector_tensor)\n",
    "\n",
    "    Mc = ((mass_1 * mass_2) ** (3 / 5)) / ((mass_1 + mass_2) ** (1 / 5))\n",
    "    eta = mass_1 * mass_2/(mass_1 + mass_2)**2.\n",
    "    A1 = Mc ** (5.0 / 6.0)\n",
    "    ci_2 = np.cos(theta_jn) ** 2\n",
    "    ci_param = ((1 + np.cos(theta_jn) ** 2) / 2) ** 2\n",
    "    \n",
    "    size = len(mass_1)\n",
    "    snr_half_ = np.zeros((len_,size))\n",
    "    d_eff = np.zeros((len_,size))\n",
    "\n",
    "    # loop over the detectors\n",
    "    for j in range(len_):\n",
    "        # loop over the parameter points\n",
    "        for i in range(size):\n",
    "            snr_half_coeff = snr_halfscaled[j]\n",
    "            snr_half_[j,i] = cubic_spline_interpolator2d(mtot[i], ratio[i], snr_half_coeff, mtot_arr, ratio_arr)\n",
    "            d_eff[j,i] =luminosity_distance[i] / np.sqrt(\n",
    "                    Fp[j,i]**2 * ci_param[i] + Fc[j,i]**2 * ci_2[i]\n",
    "                )\n",
    "\n",
    "    #amp0\n",
    "    amp0 =  A1 / d_eff\n",
    "\n",
    "    # get spin parameters\n",
    "    a_1 = np.array(params['a_1'])[idx]\n",
    "    a_2 = np.array(params['a_2'])[idx]\n",
    "    tilt_1 = np.array(params['tilt_1'])[idx]\n",
    "    tilt_2 = np.array(params['tilt_2'])[idx]\n",
    "\n",
    "    # effective spin\n",
    "    chi_eff = (mass_1 * a_1 * np.cos(tilt_1) + mass_2 * a_2 * np.cos(tilt_2)) / (mass_1 + mass_2)\n",
    "\n",
    "\n",
    "    # input data\n",
    "    XL1 = np.vstack([snr_half_[0], amp0[0], eta, chi_eff, theta_jn]).T\n",
    "    XH1 = np.vstack([snr_half_[1], amp0[1], eta, chi_eff, theta_jn]).T\n",
    "    XV1 = np.vstack([snr_half_[2], amp0[2], eta, chi_eff, theta_jn]).T\n",
    "\n",
    "    # output data\n",
    "    # get snr for y train\n",
    "    yL1 = np.array(params['L1'])[idx]\n",
    "    yH1 = np.array(params['H1'])[idx]\n",
    "    yV1 = np.array(params['V1'])[idx]\n",
    "    yNET = np.sqrt(yL1**2 + yH1**2 + yV1**2)\n",
    "\n",
    "    return(XL1, yL1, XH1, yH1, XV1, yV1, yNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phurailatpamhemantakumar/anaconda3/envs/lertest/lib/python3.10/site-packages/numba/core/ir_utils.py:2149: NumbaPendingDeprecationWarning: \n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'detector_tensor' of function 'antenna_response_array'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\n",
      "File \"../../gwsnr/njit_functions.py\", line 247:\n",
      "@njit\n",
      "def antenna_response_array(ra, dec, time, psi, detector_tensor):\n",
      "^\n",
      "\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"
     ]
    }
   ],
   "source": [
    "XL1, yL1, XH1, yH1, XV1, yV1, yNET = input_output(np.arange(len(unlensed_params['optimal_snr_net'])), unlensed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_snr_net_ANN(XL1, XH1, XV1):\n",
    "    \"\"\"\n",
    "        Function to predict the network SNR using the ANN models\n",
    "\n",
    "        Parameters:\n",
    "        XL1: input data for L1, [snr_half_[0], amp0[0], eta, chi_eff, theta_jn]\n",
    "        XH1: input data for H1, [snr_half_[1], amp0[1], eta, chi_eff, theta_jn]\n",
    "        XV1: input data for V1, [snr_half_[2], amp0[2], eta, chi_eff, theta_jn]\n",
    "\n",
    "        Returns:\n",
    "        y: network SNR\n",
    "    \"\"\"\n",
    "    x = scalerL1.transform(XL1)\n",
    "    yL1 = modelL1.predict(x)\n",
    "    yL1 = yL1 - (aL1*yL1 + bL1)\n",
    "    x = scalerH1.transform(XH1)\n",
    "    yH1 = modelH1.predict(x)\n",
    "    yH1 = yH1 - (aH1*yH1 + bH1)\n",
    "    x = scalerV1.transform(XV1)\n",
    "    yV1 = modelV1.predict(x)\n",
    "    yV1 = yV1 - (aV1*yV1 + bV1)\n",
    "    y = np.sqrt(yL1**2 + yH1**2 + yV1**2)\n",
    "\n",
    "    mass_1 = np.array(unlensed_params['mass_1'])\n",
    "    mass_2 = np.array(unlensed_params['mass_2'])\n",
    "    mtot = mass_1 + mass_2\n",
    "    y[mtot>gwsnr.mtot_max] = 0.\n",
    "\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.3222325, 1.9176006, 1.1828067, ..., 1.2374389, 2.1879537,\n",
       "       1.3156186], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_snr_net_ANN(XL1, XH1, XV1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227us/step\n",
      "[[4.62136269 4.74315895]\n",
      " [4.31896448 4.41793194]\n",
      " [5.00712204 5.29411108]\n",
      " ...\n",
      " [5.46714211 5.69066226]\n",
      " [4.16106319 3.79827401]\n",
      " [4.21725225 4.43522929]]\n"
     ]
    }
   ],
   "source": [
    "# left: predicted snr, right: actual snr\n",
    "y_pred = predict_snr_net_ANN(XL1, XH1, XV1).flatten()\n",
    "idx = (y_pred>4) & (y_pred<10)\n",
    "y_pred_ = y_pred[idx]\n",
    "y_true_ = yNET[idx]\n",
    "print(np.concatenate((y_pred_.reshape(len(y_pred_),1), y_true_.reshape(len(y_true_),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221us/step\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step\n",
      "[[49723    28]\n",
      " [   24   225]]\n",
      "Accuracy: 99.896%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred_ = predict_snr_net_ANN(XL1, XH1, XV1).flatten()\n",
    "y_test_ = yNET\n",
    "cm = confusion_matrix((y_test_>8), ((y_pred_)>8))\n",
    "print(cm)\n",
    "accuracy = accuracy_score((y_test_>8), (y_pred_>8))*100\n",
    "print(f\"Accuracy: {accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_gwsnr = gwsnr.snr(gw_param_dict=unlensed_params)['optimal_snr_net']\n",
    "snr_bilby = np.array(unlensed_params['optimal_snr_net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.61133845 4.74315895]\n",
      " [4.33200715 4.41793194]\n",
      " [4.89267168 5.29411108]\n",
      " ...\n",
      " [5.43964279 5.69066226]\n",
      " [4.22240297 3.79827401]\n",
      " [4.16183341 4.43522929]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = snr_gwsnr\n",
    "idx = (y_pred>4) & (y_pred<10)\n",
    "y_pred_ = y_pred[idx]\n",
    "y_true_ = snr_bilby[idx]\n",
    "print(np.concatenate((y_pred_.reshape(len(y_pred_),1), y_true_.reshape(len(y_true_),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49722    29]\n",
      " [   28   221]]\n",
      "Accuracy: 99.886%\n"
     ]
    }
   ],
   "source": [
    "y_pred_ = snr_gwsnr\n",
    "y_test_ = snr_bilby\n",
    "cm = confusion_matrix((y_test_>8), ((y_pred_)>8))\n",
    "print(cm)\n",
    "accuracy = accuracy_score((y_test_>8), (y_pred_>8))*100\n",
    "print(f\"Accuracy: {accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lertest",
   "language": "python",
   "name": "lertest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
